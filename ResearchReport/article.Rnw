\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package (only for this demo article)
% \usepackage{framed}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
library("MASS")
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Hanne Oberman\\Utrecht University}
   % \And Second Author\\Plus Affiliation}
\Plainauthor{Hanne Oberman}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\pkg{ShinyMICE}: an Evaluation Suite for Multiple Imputation}
\Plaintitle{ShinyMICE: an Evaluation Suite for Multiple Imputation}
\Shorttitle{\pkg{ShinyMICE}: an Evaluation Suite for Multiple Imputation}

%% - \Abstract{} almost as usual
\Abstract{
  This Research Report contains the Introduction and Methods section of the technical paper that will be submitted for publication in \emph{Journal of Statistical Software}. ("There is no page limit, nor a limit on the number of figures or tables", see \url{https://www.jstatsoft.org/pages/view/authors}.) I have chosen to use the second format from the Research Report guidelines: "It is the first half of the thesis, i.e., there are no results included yet, but the report contains a full introduction including a literature review and a methods section that contains details about the data, instruments and or statistical procedures". The goal was to develop novel methodology and guidelines for evaluating multiple imputation methods, and implement these in an interactive evaluation framework for multiple imputation: \pkg{ShinyMICE}.
  }

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{multiple imputation, evaluation methodology, \pkg{ShinyMICE}, \pkg{mice}, \proglang{R}}
\Plainkeywords{multiple imputation, evaluation methodology, ShinyMICE, mice, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Hanne Ida Oberman, BSc.\\
  Methodology and Statistics for the Behavioural, Biomedical and Social Sciences\\
  Department of Methodology and Statistics\\
  Faculty of Social and Behavioral Sciences\\
  Utrecht University\\
  Heidelberglaan~15\\
  3500 Utrecht, The Netherlands\\
  E-mail: \email{h.i.oberman@uu.nl} % \\
  % URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, \fct{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section[Introduction: Multiple Imputation Methodology]{Introduction: Multiple Imputation Methodology} \label{sec:intro} % curly brackets can handle typesetting

% \begin{leftbar}
% The introduction is in principle ``as usual''. However, it should usually embed
% both the implemented \emph{methods} and the \emph{software} into the respective
% relevant literature. For the latter both competing and complementary software
% should be discussed (within the same software environment and beyond), bringing
% out relative (dis)advantages. All software mentioned should be properly
% \verb|\cite{}|d. (See also Appendix~\ref{app:bibtex} for more details on
% \textsc{Bib}{\TeX}.) Example: % \citep[][Chapter~7.4]{Venables+Ripley:2002} 
% 
% For writing about software JSS requires authors to use the markup
% \verb|\proglang{}| (programming languages and large programmable systems),
% \verb|\pkg{}| (software packages), \verb|\code{}| (functions, commands,
% arguments, etc.). If there is such markup in (sub)section titles (as above), a
% plain text version has to be provided in the {\LaTeX} command as well. Below we
% also illustrate how abbrevations should be introduced and citation commands can
% be employed. See the {\LaTeX} code for more details.
% \end{leftbar}

At some point, any scientist conducting statistical analyses will run into a missing data problem \citep{alli02}. Missingness is problematic because statistical inference cannot be performed on incomplete data, and  ad hoc solutions can yield wildly invalid results \citep{buur18}. To circumvent the ubiquitous problem of missing information, \cite{rubin87} proposed the framework of multiple imputation (MI). MI is an iterative algorithmic procedure in which missing data points are 'guessed' (i.e. imputed) several times. The variability between the imputations validly reflects how much uncertainty in the inference is due to missing information--that is, if all statistical assumptions are met \cite{rubin87}.

With MI, many assumptions are made about the nature of the observed and missing parts of the data and their relation to the 'true' \emph{data generating model} \citep{buur18}. Without proper evaluation of the imputations and the underlying assumptions, any drawn inference may erroneously be deemed valid. Such evaluation measures are currently missing or under-developed in MI software, like the world leading \proglang{R} package \pkg{mice} \citep{mice11}. Therefore, I will answer the following question: 'Which measures are vital for evaluating the validity of multiply imputed data?'.

%% -- Features ---------------------------------------------------------------
\subsection{Features} \label{sec:features}

All programming code used in this paper is available in the file XYZ.R along with the manuscript and on Github repository XYZ. 

"The intended audience of this paper consists of applied researchers who want to address problems caused by missing data by multiple imputation. The text assumes basic familiarity with R. The document contains hands-on analysis using the mice package. We do not discuss problems of incomplete data in general. We refer to the excellent books by Little and Rubin (2002) and Schafer (1997). Theory and applications of multiple imputation have been developed in Rubin (1987) and Rubin (1996). van Buuren (2012) introduces multiple imputation from an applied perspective" \cite[p.~4]{mice}.

%% -- Notation ---------------------------------------------------------------
\subsection{Notation and conventions?} \label{sec:notation}

Notation in this paper is copied from \cite{buur18}, and thus based on \cite{rubin87}.

% "Let Y denote the n × p matrix containing the data values on p variables for all n units in the sample. We define the response indicator R as an n × p 0-1 matrix. The elements of Y and R are denoted by y i j and r i j , respectively, where i = 1 , ... , n and j = 1 , ... , p . If y i j is observed, then r i j = 1 , and if y i j is missing, then r i j = 0 ." ... The observed data are collectively denoted by Y o b s . The missing data are collectively denoted as Y m i s , and contain all elements y i j where r i j = 0 . When taken together Y = ( Y o b s , Y m i s ) contain the hypothetically complete data." \cite[par.~2.2]{buur18}

Terminology (MCAR, MAR, MNAR)

% The data are said to be MCAR if Pr ( R = 0 | Y o b s , Y m i s , ?? ) = Pr ( R = 0 | ?? ) (2.1) so the probability of being missing depends only on some parameters ?? , the overall probability of being missing. The data are said to be MAR if Pr ( R = 0 | Y o b s , Y m i s , ?? ) = Pr ( R = 0 | Y o b s , ?? ) (2.2) so the missingness probability may depend on observed information, including any design factors. Finally, the data are MNAR if Pr ( R = 0 | Y o b s , Y m i s , ?? ) (2.3) does not simplify, so here the probability to be missing also depends on unobserved information, including Y m i s itself. \cite[par.~2.2]{buur18}

Blue points are observed, the red points are imputed. 



%% -- Theoretical Background ---------------------------------------------------------------
\section{Theoretical Background} \label{sec:background}

- missingness mechanims, ignorability

"The practical importance of the distinction between MCAR, MAR and MNAR is that it clarifies the conditions under which we can accurately estimate the scientifically interesting parameters without the need to know $\psi$" \cite[par.~2.2]{buur18}.

- Rubin's rules

- FCS vs. JM?

The validity of the MI solution depends on numerous assumptions that cannot be verified from the observed data alone. So instead of statistical tests for assumptions, evaluation procedures have been developed. For the following assumptions, no reliable procedure has been proposed and/or implemented: 1) \emph{ignorability} of the \emph{missingness mechanism} \citep{rubin87}; 2) \emph{congeneality} of the imputation models \citep{meng94}; and 3) \emph{compatibility} of the MI modeling procedure \citep{rubin96}. 

\begin{enumerate}
\item A missingness mechanism is said to be ignorable when the probability to be missing does not depend on the missing data itself. Violation of this assumption can gravely affect inferences. Robustness of inferences to varying degrees of violation can be assessed with sensitivity analyses. Some practical guidelines exist (e.g., \citep{nguy17}), but current MI software does not facilitate this methodology for empirical researchers. 
%
\item Congeneal imputation models capture all required relations between observed and missing parts of the data. The extent to which this has been successful can be evaluated by plotting conditional distributions \citep{abay08}. Such visualizations are available in MICE, but subsequent statistical tests to quantify the relations with covariates are not provided. 
% ^[Additionally, there is potential to assess model fit by means of \emph{over-imputation} \citep{buur18} or *double robustness* \citep{bang05}-- topics that are only persued if time permits.] 
%
\item The third assumption is met when the MI algorithm converges to a stable distribution. However, conventional measures to diagnose convergence-- e.g., Gelman and Rubin's \citeyear{gelm92} statistic $\widehat{R}$ --are not applicable on multiply imputed data \citep{lace07}. Therefore, empirical researchers have to rely on visual inspection procedures that are theoretically equivalent to $\widehat{R}$ \citep{whit11}. Visually assessing convergence is not only difficult to the untrained eye, it might also be futile. The convergence properties of MI algorithms lack scientific consensus \citep{taka17}, and some default MICE techniques might not converge to stable distributions at all \citep{murr18}. Moreover, convergence diagnostics for MI methods have not been systematically studied \citep{buur18}.
\end{enumerate}

In short, the existing literature provides both possibilities and limitations to evaluating the validity of multiply imputed data. The goal of this research project is to develop novel methodology and guidelines for evaluating MI methods, and implement these in an interactive evaluation framework for multiple imputation. This framework will aid applied researchers in drawing valid inference from incomplete datasets. 


%% -- Methods ---------------------------------------------------------------

%%
%%
%%
%%

\section{Methods} \label{sec:methods}

Initially, the research project will consist of an investigation into algorithmic convergence of MI algorithms. I will replicate Lacerda et al.'s simulation study on $\widehat{R}$ \citep{lace07}, and develop novel guidelines for assessing convergence. Ideally, I will integrate several diagnostics (e.g., $\widehat{R}$, \emph{auto-correlation}, and \emph{simulation error}) into a single summary indicator to flag non-convergence. 

Subsequently, I will use\proglang{R}Shiny \citep{shiny} to implement the convergence indicator and existing evaluation measures in \pkg{ShinyMICE}, see Figure 1. The application will at least contain methodology for: sensitivity analyses; data visualizations (e.g., scatter-plots, densities, cross-tabulations); and statistical evaluation of relations between variables pre- versus post-imputation (i.e., $\chi^2$~tests or $t$~tests).

A working beta version of \pkg{ShinyMICE} will be considered a sufficient milestone to proceed with writing a technical paper on the methodology and the software. I will submit the paper for publication in \emph{Journal of Statistical Software}. Finally, \pkg{ShinyMICE} will be integrated into the existing MICE environment, and a vignette for applied researchers will be written. 

The \proglang{R} code and documentation of this project will be open source (available on Github). Since the study does not require the use of unpublished empirical data, I expect that the FETC will grant the label exempt. 

% \includegraphics[height=80mm,keepaspectratio]{logo} % this is not correct yet, it needs a figure label and stuff


%% -- Methods: Sensitivity Analyses ---------------------------------------------------------------
\subsection{Sensitivity Analyses} \label{sec:sensitivity}

- Robustness of inferences to varying degrees of violation can be assessed with sensitivity analyses. See \citep{nguy17}.

- Forked MICE with MNAR sensitivity analyses, see \url{https://thestatsgeek.com/2018/06/07/missing-not-at-random-sensitivity-analysis-with-fcs-multiple-imputation/}.

See https://stefvanbuuren.name/fimd/sec-sensitivity.html

"The MAR assumption can never be tested from the observed data. One can however check whether the imputations created by MICE algorithm are plausible" \cite[p.~42]{mice}. 

"Plot densities of both the observed and imputed values of all variables to see whether the imputations are reasonable. Differences in the densities between the observed and imputed values may suggest a problem that needs to be further checked" \cite[p.~43]{mice}. 


%% -- Methods: Visualizations ---------------------------------------------------------------
\subsection{Data Visualization} \label{sec:visualization}

- Implement existing plot functions in \pkg{ShinyMICE}, add subsequent statistical tests to quantify the relations with covariates.

- Relations between observed and missing parts of the data: use \pkg{lattice} package functionalities to improve the \pkg{mice} functions \fct{hist} and \fct{xyplot}.

Create scatterplots via \fct{xyplot} and use lattice functionalities to adapt.

"An important step in multiple imputation is to assess whether imputations are plausible. Imputations should be values that could have been obtained had they not been missing. Imputations should be close to the data. Data values that are clearly impossible (e.g., negative counts, pregnant fathers) should not occur in the imputed data. Imputations should respect relations between variables, and reflect the appropriate amount of uncertainty about their 'true' values. Diagnostic checks on the imputed data provide a way to check the plausibility of the imputations" \cite[p.~11]{mice}.

"The mice package contains several graphic functions that can be used to gain insight into the correspondence of the observed and imputed data: \fct{bwplot}, \fct{stripplot}, \fct{densityplot} and \fct{xyplot}." \cite[par.~6.6]{buur18}

see https://onlinelibrary.wiley.com/doi/full/10.1111/1467-9574.00219

"It is often useful to inspect the distributions of original and the imputed data. One way of doing this is to use the function \fct{stripplot} \dots  The differences between the
red points represents our uncertainty about the true (but unknown) values. \dots Under MCAR, univariate distributions of the observed and imputed data are expected to be identical. Under MAR, they can be different, both in location and spread, but their multivariate distribution is assumed to be identical." \cite[p.~12]{mice}. 

"Bondarenko and Raghunathan (2016) proposed a more refined diagnostic tool that aims to compare the distributions of observed and imputed data conditional on the missingness probability. The idea is that under MAR the conditional distributions should be similar if the assumed model for creating multiple imputations has a good fit. An example is created as ..."  \cite[par.~6.6]{buur18}

see https://www.ncbi.nlm.nih.gov/pubmed/26952693

% fit <- with(imp, glm(ici(imp) ~ age + bmi + hyp + chl,
%                      family = binomial))
% ps <- rep(rowMeans(sapply(fit$analyses, fitted.values)),
%           imp$m + 1)
% xyplot(imp, bmi ~ ps | as.factor(.imp),
%        xlab = "Probability that record is incomplete",
%        ylab = "BMI", pch = c(1, 19), col = mdc(1:2))


%% -- Methods: Convergence ---------------------------------------------------------------
\subsection{Convergence Diagnostic} \label{sec:convergence}

- Convergence issues: "Imputers who do choose to use FCS should use flexible univariate models wherever possible and take care to assess apparent convergence of the algorithm, for example by computing traces of pooled estimates or other statistics and using standard MCMC diagnostics (Gelman et al., 2013, Chapter 11). It may also be helpful to examine the results of many independent runs of the algorithm with different initializations and to use random scans over the p variables to try to identify any convergence issues and mitigate possible order dependence" \cite[p.~19]{murr18}.

- Replicate simulation study and build a decision rule to solve the problem: "The monitoring statistic was computed for mean monthly earnings at each iteration in chains of length k = 200. Since calculation of the statistic requires M parallel sequences, m = 5 such chains were constructed. This value of m was informed by the preferred choice given in the literature on multiple imputation. The monitoring statistic computed at each iteration is presented in Figures 15 to 17 for each of the three missingness mechanisms. The red vertical line denotes ten iterations" \cite[p.~49]{lace07}. % also, use '\citep[see][for further details]{...}' for '(see Author Year for further details)'


- Potentially add something about updated (2019) version of $\widehat{R}$ and the new threshold of 1.01, see \citep{veht19}.

- Auto-correlation: "Applications of MICE with lowly correlated data therefore inject a lot of noise into the system. Hence, the auto-correlation over t will be low, and convergence will be rapid, and in fact immediate if all variables are independent. Thus, the incorporation of noise into the imputed data has pleasant side-effect of speeding up convergence" \citep{buur18}, par. 4.5. Also, schafer (1997, p. 129) wrote on worst linear statistic. We could calculate the autocorrelation of that statistic to know that the algorithm converged elsewhere too. See autocorr function plot in sas of worst linear function. Note: we're talking about missing data only, not the combined data (that autocorrelation is very high, as is the autocorrelation of deductively imputed values, like in the texp example). Worst linear function in SAS see \url{https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mi_sect027.htm}.

- Stability of the solution: possibly use the slope of means over iterations too to see whether there is trending. Or apply PCA on the imputed data and if that (the eigenvalues?) stays the same we know that the means and variances are stable as well. Or look at MC error: MC error = SD/sqrt(number of iterations), where SD represents the variation across iterations. The MC error thus represents how much the means differ w.r.t. the iterations. MC error decreases as number of iterations increases. It should not be larger than 5\% of the sample standard deviation.

"In general, you cannot know for sure if your chain has converged. But sometimes you can know if your chain has not converged, so we at least check for this latter possibility" \cite[p.~101]{hoff09}




%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)

\section{Models and software} \label{sec:models}

\emph{This is a placeholder.}


% A count response variable $y_i$ ($i = 1, \dots, n$) by assuming a Poisson distribution
% $y_i \sim \mathrm{Pois}(\mu_i)$. The dependence of the conditional mean
% $\E[y_i \, | \, x_i] = \mu_i$ on the regressors $x_i$ is then specified via a
% log link and a linear predictor
% %
% \begin{equation} \label{eq:mean}
% \log(\mu_i) \quad = \quad x_i^\top \beta,
% \end{equation}
% %
% where the regression coefficients $\beta$ are estimated by maximum likelihood
% (ML) using the iterative weighted least squares (IWLS) algorithm.

% \begin{leftbar}
% Note that around the \verb|{equation}| above there should be no spaces (avoided
% in the {\LaTeX} code by \verb|%| lines) so that ``normal'' spacing is used and
% not a new paragraph started.
% \end{leftbar}

% Its most important arguments are
% \begin{Code}
% glm(formula, data, subset, na.action, weights, offset,
%   family = gaussian, start = NULL, control = glm.control(...),
%   model = TRUE, y = TRUE, x = FALSE, ...)
% \end{Code}
% where \code{formula} plus \code{data} is the now standard way of specifying
% regression relationships in \proglang{R}/\proglang{S} introduced in
% \cite{Chambers+Hastie:1992}. The remaining arguments in the first line
% (\code{subset}, \code{na.action}, \code{weights}, and \code{offset}) are also
% standard  for setting up formula-based regression models in
% \proglang{R}/\proglang{S}. The arguments in the second line control aspects
% specific to GLMs while the arguments in the last line specify which components
% are returned in the fitted model object (of class \class{glm} which inherits
% from \class{lm}). For further arguments to \fct{glm} (including alternative
% specifications of starting values) see \code{?glm}. For estimating a Poisson
% model \code{family = poisson} has to be specified.

% \begin{leftbar}
% As the synopsis above is a code listing that is not meant to be executed,
% one can use either the dedicated \verb|{Code}| environment or a simple
% \verb|{verbatim}| environment for this. Again, spaces before and after should be
% avoided.
% 
% Finally, there might be a reference to a \verb|{table}| such as
% Table~\ref{tab:overview}. Usually, these are placed at the top of the page
% (\verb|[t!]|), centered (\verb|\centering|), with a caption below the table,
% column headers and captions in sentence style, and if possible avoiding vertical
% lines.
% \end{leftbar}

% \begin{table}[t!]
% \centering
% \begin{tabular}{lllp{7.4cm}}
% \hline
% Type           & Distribution & Method   & Description \\ \hline
% GLM            & Poisson      & ML       & Poisson regression: classical GLM,
%                                            estimated by maximum likelihood (ML) \\
%                &              & Quasi    & ``Quasi-Poisson regression'':
%                                            same mean function, estimated by
%                                            quasi-ML (QML) or equivalently
%                                            generalized estimating equations (GEE),
%                                            inference adjustment via estimated
%                                            dispersion parameter \\
%                &              & Adjusted & ``Adjusted Poisson regression'':
%                                            same mean function, estimated by
%                                            QML/GEE, inference adjustment via
%                                            sandwich covariances\\
%                & NB           & ML       & NB regression: extended GLM,
%                                            estimated by ML including additional
%                                            shape parameter \\ \hline
% Zero-augmented & Poisson      & ML       & Zero-inflated Poisson (ZIP),
%                                            hurdle Poisson \\
%                & NB           & ML       & Zero-inflated NB (ZINB),
%                                            hurdle NB \\ \hline
% \end{tabular}
% \caption{\label{tab:overview} Overview of various count regression models. The
% table is usually placed at the top of the page (\texttt{[t!]}), centered
% (\texttt{centering}), has a caption below the table, column headers and captions
% are in sentence style, and if possible vertical lines should be avoided.}
% \end{table}


%% -- Illustrations ------------------------------------------------------------

%% - Virtually all JSS manuscripts list source code along with the generated
%%   output. The style files provide dedicated environments for this.
%% - In R, the environments {Sinput} and {Soutput} - as produced by Sweave() or
%%   or knitr using the render_sweave() hook - are used (without the need to
%%   load Sweave.sty).
%% - Equivalently, {CodeInput} and {CodeOutput} can be used.
%% - The code input should use "the usual" command prompt in the respective
%%   software system.
%% - For R code, the prompt "R> " should be used with "+  " as the
%%   continuation prompt.
%% - Comments within the code chunks should be avoided - these should be made
%%   within the regular LaTeX text.

\section{Illustrations} \label{sec:illustrations}

\emph{This is a placeholder.}

% For a simple illustration of basic Poisson and NB count regression the
% \code{quine} data from the \pkg{MASS} package is used. This provides the number
% of \code{Days} that children were absent from school in Australia in a
% particular year, along with several covariates that can be employed as regressors.
% The data can be loaded by
%
% <<data>>=
% data("quine", package = "MASS")
@
%
% and a basic frequency distribution of the response variable is displayed in
% Figure~\ref{fig:quine}.

% \begin{leftbar}
% For code input and output, the style files provide dedicated environments.
% Either the ``agnostic'' \verb|{CodeInput}| and \verb|{CodeOutput}| can be used
% or, equivalently, the environments \verb|{Sinput}| and \verb|{Soutput}| as
% produced by \fct{Sweave} or \pkg{knitr} when using the \code{render_sweave()}
% hook. Please make sure that all code is properly spaced, e.g., using
% \code{y = a + b * x} and \emph{not} \code{y=a+b*x}. Moreover, code input should
% use ``the usual'' command prompt in the respective software system. For
% \proglang{R} code, the prompt \code{"R> "} should be used with \code{"+  "} as
% the continuation prompt. Generally, comments within the code chunks should be
% avoided -- and made in the regular {\LaTeX} text instead. Finally, empty lines
% before and after code input/output should be avoided (see above).
% \end{leftbar}

% \begin{figure}[t!]
% \centering
% <<visualization, echo=FALSE, fig=TRUE, height=5.2, width=7>>=
% par(mar = c(4, 4, 1, 1))
% plot(table(quine$Days), xlab = "Days", ylab = "Frequency", axes = FALSE)
% axis(2)
% axis(1, at = 0:16 * 5, labels = FALSE)
% axis(1, at = 0:8 * 10)
% box()
@
% \caption{\label{fig:quine} Frequency distribution for number of days absent
% from school.}
% \end{figure}

% As a first model for the \code{quine} data, we fit the basic Poisson regression
% model. (Note that JSS prefers when the second line of code is indented by two
% spaces.)
%
% <<poisson>>=
% m_pois <- glm(Days ~ (Eth + Sex + Age + Lrn)^2, data = quine,
%   family = poisson)
@
%
% To account for potential overdispersion we also consider a negative binomial
% GLM.
%
% <<negbin>>=
% library("MASS")
% m_nbin <- glm.nb(Days ~ (Eth + Sex + Age + Lrn)^2, data = quine)
@
%
% In a comparison with the BIC the latter model is clearly preferred.
% %
% <<comparison>>=
% BIC(m_pois, m_nbin)
@
% %
% Hence, the full summary of that model is shown below.
% %
% <<summary>>=
% summary(m_nbin)
@



%% -- Summary/conclusions/discussion -------------------------------------------

\section{Summary and discussion} \label{sec:summary}

\emph{This is a placeholder.}

% \begin{leftbar}
% As usual \dots
% \end{leftbar}


%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

% \begin{leftbar}
% If necessary or useful, information about certain computational details
% such as version numbers, operating systems, or compilers could be included
% in an unnumbered section. Also, auxiliary packages (say, for visualizations,
% maps, tables, \dots) that are not cited in the main text can be credited here.
% \end{leftbar}

The results in this paper were obtained using \proglang{R}~\Sexpr{paste(R.Version()[6:7], collapse = ".")} with the \pkg{mice}~\Sexpr{packageVersion("mice")} package. \proglang{R} itself and all packages used are available from the Comprehensive \proglang{R} Archive Network (CRAN) at \url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}

% \begin{leftbar}
% All acknowledgments (note the AE spelling) should be collected in this
% unnumbered section before the references. It may contain the usual information
% about funding and feedback from colleagues/reviewers/etc. Furthermore,
% information such as relative contributions of the authors may be added here
% (if any).
% \end{leftbar}

This technical paper is written by the sole author (Hanne Oberman, BSc.), with guidance from Master thesis supervisors prof. dr. Stef van Buuren, and dr. Gerko Vink.

%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{ShinyMICE}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

\begin{appendix}

\section{More technical details} \label{app:technical}

% \begin{leftbar}
% Appendices can be included after the bibliography (with a page break). Each
% section within the appendix should have a proper section title (rather than
% just \emph{Appendix}).
% 
% For more technical style details, please check out JSS's style FAQ at
% \url{https://www.jstatsoft.org/pages/view/style#frequently-asked-questions}
% which includes the following topics:
% \begin{itemize}
%   \item Title vs.\ sentence case.
%   \item Graphics formatting.
%   \item Naming conventions.
%   \item Turning JSS manuscripts into \proglang{R} package vignettes.
%   \item Trouble shooting.
%   \item Many other potentially helpful details\dots
% \end{itemize}
% \end{leftbar}


\section[Using BibTeX]{Using \textsc{Bib}{\TeX}} \label{app:bibtex}

% \begin{leftbar}
% References need to be provided in a \textsc{Bib}{\TeX} file (\code{.bib}). All
% references should be made with \verb|\cite|, \verb|\citet|, \verb|\citep|,
% \verb|\citealp| etc.\ (and never hard-coded). This commands yield different
% formats of author-year citations and allow to include additional details (e.g.,
% pages, chapters, \dots) in brackets. In case you are not familiar with these
% commands see the JSS style FAQ for details.
% 
% JSS requires the following format.
% \begin{itemize}
%   \item JSS-specific markup (\verb|\proglang|, \verb|\pkg|, \verb|\code|) should
%     be used in the references.
%   \item Titles should be in title case.
%   \item Journal titles should not be abbreviated and in title case.
%   \item DOIs should be included where available.
%   \item Software should be properly cited as well. For \proglang{R} packages
%     \code{citation("pkgname")} typically provides a good starting point.
% \end{itemize}
% \end{leftbar}

\end{appendix}

%% -----------------------------------------------------------------------------


\end{document}

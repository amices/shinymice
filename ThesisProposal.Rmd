---
title: "Thesis Proposal"
author: "Hanne Oberman (4216318)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  #word_document: default
  pdf_document:
    fig_caption: yes
subtitle: Methodology and Statistics for the Behavioural, Biomedical and Social Sciences
bibliography: ThesisProposal.bib
csl: acm.csl

---

\centering

# ShinyMICE: an Evaluation Suite for Multiple Imputation

Supervised by prof. dr. Stef van Buuren, & dr. Gerko Vink

Department of Methodology and Statistics 

Utrecht University

Word count: 739


\raggedright 

<!-- Requirements: max 750 words (excl. references) -->

<!-- In general the proposal should focus on the relevancy of the project, the gap in the scientific literature, and the feasibility to finish the project within 8 months. Possibly specify a 'step 2' to pursue after the main objective is reached. -->


\newpage


# Introduction

At some point, any scientist conducting statistical analyses will run into a missing data problem [@alli02]. Missingness is problematic because statistical inference cannot be performed on incomplete data, and  ad hoc solutions can yield wildly invalid results [@buur18]. To circumvent the ubiquitous problem of missing information, Rubin [@rubin87] proposed the framework of multiple imputation (MI). MI is an iterative algorithmic procedure in which missing data points are 'guessed' several times. The variability between these 'guesses' validly reflects how much uncertainty in the inference is due to missing information--that is, if all statistical assumptions are met [@rubin87].

With MI, many assumptions are made about the nature of the observed and missing parts of the data, and their relation to the 'true' data generating model [@buur18]. Without proper evaluation of the imputations and the underlying assumptions, any drawn inference may erroneously be deemed valid. Such evaluation measures are currently missing or under-developed in MI software. Therefore, I aim to answer the following question: 'Which measures are vital for evaluating the validity of multiply imputed data?'. 

The goal is to develop an MI evaluation suite for the world leading R package `MICE` [@mice11]. This research project will aid applied researchers in drawing valid inference from incomplete data sets. Simultaneously, a contribution to the scientific literature is made by developing novel methodology and guidelines for evaluating MI data methods. 


# Literature Review

The validity of MI data depends on numerous assumptions that--by definition--cannot be verified from the incomplete data. Therefore, existing evaluation methods rely on proxy measures. For the following assumtions, no reliable proxy measures have been proposed and/or implemented in `MICE`: 1) *ignorability* of the *missingness mechanism*; 2) suitability of the *imputation models*; and 3) *compatibility* with the MI algorithm. 
<!-- Rubin (1987b, 160-66) subdivided the work needed to create imputations into three tasks. The modeling task chooses a specific model for the data, the estimation task formulates the posterior parameters distribution given the model and the imputation task takes a random draws for the missing data by drawing successively from parameter and data distributions @buur18 par 4.5.1 -->

<!-- account for the process that created the missing data, -->
<!-- preserve the relations in the data, and -->
<!-- preserve the uncertainty about these relations. -->


<!-- The modeling task chooses a specific model for the data -> assumes ignorability. -->

<!-- The estimation task computes the posterior distribution of the model parameters of the assumes correct specification of the model. ->

<!-- The imputation task draws plausible values from the conditional posterior distribution of the missing data given the estimated parameters -> assumes convergence in distribution -->


1. The missingness mechanism is said to be ignorable when the 'cause' of the missingness does not depend on the missing data itself [@rubin87]. Evaluation is usually done with sensitivity analyses to assess the robustness of inferences to violations. Some practical guidelines exist (e.g., [@nguy17]), but current MI software does not facilitate this methodology for empirical researchers. 


<!-- **miss mech is model hoe obs and miss samenhangen, draait om ignorability =  op moment dat je obs kunt gebruiken om miss data te schatten. hoeft niet hetzelfde te zijn. relatie kunnen we uit de data halen, ondanks dat we de ontbrekende waaren zelf kunnen negeren, alleen het mech dat die waarden heeft gegenereerd. ** -->

2. The imputation models should capture the relations between observed and missing parts of the data. These relations can be evaluated by plotting conditional distributions [@abay08]. Such visualizations are available in `MICE`, but subsequent statistical tests to quantify the relations with covariates are not yet provided. Additionally, there is potential to assess model fit by means of *over-imputation* [@buur18] or *double robustness* [@bang05]. 

<!-- **er zijn 3 modellen: analyse, imputatie, non-response model (of missing data model). in mice wordt iedere var voorspeld uit andere vars. eerst analysemodel, is scientific mod of int. als je dat niet meeneemt dan mis je ruimte in alg om datgene dat je probeert te vinden mee te nemen (non-comp). non-resp is verklaren waarom missings in een var vaker voorkomen dan andere vars. meng's 1994 paper on compatibility and congeneality.** -->

3. The third assumption is met when the MI algorithm converges to a stable distribution. Conventional measures to diagnose convergence--e.g., Gelman and Rubin's [-@gelm92] statistic $\widehat{R}$--do not suit multiply imputed data [@lace07]. Therefore, empirical researchers have to rely on a visual inspection procedure that is theoretically equivalent to $\widehat{R}$ [@whit11]. Not only is visually assessing convergence difficult for the untrained eye, it might also be futile. The convergence properties of MI algorithms lack scientific consensus [@taka17], and some default `MICE` techniques might not converge to stable distributions at all [@murr18]. Moreover, convergence diagnostics for MI methods have not been systematically studied [@buur18].

In short, the existing literature provides both possibilities and limitations to evaluating the validity of multiply imputed data. My aim is to (further) develop and implement the methodology.


# Methods

This research project will be supervised by the `MICE` developers. I will develop novel methodology for evaluating MI data, and implement these methods using R Shiny [@shiny17]. The endproduct is an interactive evaluation device: '`ShinyMICE`'. The R code and documentation will be open source (available on Github). Since this project does not require the use of unpublished empirical data, I expect that the FETC will grant this project the label 'exempt'. 

The research report will consist of an investigation into algorithmic convergence of MI algorithms. I will replicate Lacerda et al.'s simulation study on $\widehat{R}$ [@lace07], and potentially combine this convergence measure with auto-correlation or simulation error diagnostics. Ideally, I will develop a single summary indicator to flag non-convergence. 

Subsequently, I will implement this and other evaluation measures in `ShinyMICE`, see Figure 1. The application will at least contain methodology for: sensitivity analyses; data visualizations (e.g., scatter-plots, densities, cross-tabulations); and statistical evaluations of relations between variables pre- versus post-imputation (i.e., $\chi^2$-tests or $t$-tests).     

![Preliminary impression of the interactive `ShinyMICE` user interface.](Figures/Impression.png)

A working beta version of `ShinyMICE` will be considered a sufficient milestone to proceed with writing a technical paper on the methodology and the software. I aim to submit this for publication in *Journal of Statistical Software*. Finally, `ShinyMICE` will be integrated into the existing `MICE` environment, and a vignette for applied researchers will be written. 




# References


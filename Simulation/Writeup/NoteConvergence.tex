\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package (only for this demo article)
% \usepackage{framed}
\usepackage{multirow, amsmath, tikz}
\usetikzlibrary{fit, positioning}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}



%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{\\Research Report (Preparation for Research Master Thesis, Research Seminar) \\ Methodology and Statistics for the Behavioural, Biomedical and Social Sciences \AND Hanne Oberman\\Utrecht University}
   % \And Second Author\\Plus Affiliation}
\Plainauthor{Hanne Oberman}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{A Note on Convergence Diagnostics for Multiple Imputation Algorithms}
\Plaintitle{A Note on Convergence Diagnostics for Multiple Imputation Algorithms}
\Shorttitle{Convergence Diagnostics for MI}

%% - \Abstract{} almost as usual
\Abstract{
The ubiquitous problem of missing information may be circumvented by implementing the framework of multiple imputation (MI). With MI, missing data points are `filled in' (i.e. imputed) several times, and statistical inference is performed on each imputed data set. The validity of inferences may be threatened by non-convergence of this iterative algorithmic procedure, but diagnostics to evaluate algorithmic convergence have not been implemented in MI software. This note investigates the performance of two convergence diagnostics (\citep[$\widehat{R}$,][]{gelm92}; \citep[autocorrelation,][]{scha97}) through model-based simulation. To investigate convergence of the algorithm, the number of iterations is varied between one and one-hundred. Each of these 100 simulation conditions is replicated 1000 times. The convergence diagnostics are evaluated against three recommended simulation diagnostics (bias, confidence interval width, and coverage rate \citep{buur18}, since no benchmark diagnostic is available. This simulation study shows that three iterations may be sufficient to obtain ... \textbf{This Research Report contains a simulation study that serves as the basis of the technical paper that will be submitted for publication in \emph{Journal of Statistical Software}. I aim to publish a pre-print of this research report on \href{https://arxiv.org/}{arXiv}. That way, I can refer to the simulation study described here, without `bulking up' the technical paper that I want to submit for publication in JSS. Another option would be to attach this note as an online appendix to the technical paper on ShinyMICE.}  
% This Research Report contains a simulation study that serves as the basis of the technical paper that will be submitted for publication in \emph{Journal of Statistical Software}. I have chosen to use the first format from the Research Report guidelines: ``\emph{It is written as a (mini) thesis, with an introduction, methods section, some results (i.e., preliminary analyses, or pilot simulations), and a discussion of results. The length of the research report should be maximally 2500 words of text (without references list and or tables and figures). Please do not include appendices, and no more than 6 tables or figures. Table and Figure captions do not count towards the word limit. An abstract may be included, but is not necessary}''. I aim to publish a pre-print of this research report on \href{https://arxiv.org/}{ArXiv}. That way, I can refer to the simulation study described here, without 'bulking up' the technical paper that I want to submit for publication in JSS. Another option would be to attach this note as online appendix to the technical paper on ShinyMICE.
  }

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{multiple imputation, convergence, \pkg{mice}, \proglang{R}}
\Plainkeywords{multiple imputation, convergence, mice, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Hanne Ida Oberman, BSc.\\
  Methodology and Statistics for the Behavioural, Biomedical and Social Sciences\\
  Department of Methodology and Statistics\\
  Faculty of Social and Behavioral Sciences\\
  Utrecht University\\
  Heidelberglaan~15, 3500 Utrecht, The Netherlands\\
  E-mail: \email{h.i.oberman@uu.nl} % \\
  URL: \url{https://hanneoberman.github.io/}
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

\section{Introduction} \label{sec:intro} % curly brackets can handle typesetting

At some point, any scientist conducting statistical analyses will run into a missing data problem \citep{alli02}. Missingness is problematic because statistical inference cannot be performed on incomplete data without employing \emph{ad hoc} solutions (e.g., list-wise deletion), which may yield wildly invalid results \citep{buur18}. A popular answer to the ubiquitous problem of missing information is to use the framework of multiple imputation (MI), proposed by \cite{rubin87}. MI is an iterative algorithmic procedure in which each missing data point is `imputed' (i.e. filled in) several times. The variability between imputations is used to reflect how much uncertainty in the inference is introduced by the missingness. Therefore, MI can provide valid inferences despite missing information. 

To obtain valid inferences with MI, the variability between imputations should be properly represented \citep{rubin87, buur18}. If this variability is under-estimated, confidence intervals around estimates will be too narrow, which can yield spurious results. However, over-estimation of the variance between imputations results in unnecessarily wide confidence intervals, which can be costly because it lowers the statistical power. Since both of these situations are undesirable, imputations and their variability should be evaluated. Evaluation measures, however, are currently missing or under-developed in MI software, like the world-leading \pkg{mice} package \citep{mice} in \proglang{R} \citep{R}. %\textbf{ADD sentence to link eval measures to convergence!}
%Therefore, I will answer the following question: 'Which measures are vital for evaluating the validity of multiply imputed data?'.

The goal of this research project is to develop novel methodology and guidelines for evaluating MI methods. These tools will subsequently be implemented in an interactive evaluation framework for multiple imputation to aid applied researchers in drawing valid inference from incomplete datasets. This note provides the theoretical foundation towards the diagnostic evaluation of the convergence of MI algorithms. % for diagnostics for evalua one of these evaluation measures that is vital in evaluating MI procedures: a diagnostic to assess convergence of the MI algorithm. 
For reasons of brevity, this note only focuses on the MI algorithm implemented in \pkg{mice} \cite{mice}. %The implementation limits itself to the MI algorithm 
%More specifically, this note focuses on the MI algorithm that is implemented in the \proglang{R} package \pkg{mice}: `Multiple Imputation using Chained Equations' (MICE) \textbf{[can be removed, can be found in the mice ref]}. 
To this end, we will evaluate how convergence of the imputation algorithm can be diagnosed.

%The research question that will be addressed is: %. a measure that is vital in evaluating one vital measure for evaluating MI procedures--an algorithmic convergence diagnostic. This note specifically investigates one vital evaluation measure: how to diagnose convergence of a multiple imputation algorithm. 
%This note focusses on one measure that is vital for evaluating the validity of multiply imputed data: a convergence diagnostic. of the MI algorithm. 
%Therefore, the aim of this research project is to investigate which measures are vital for . how the convergence properties of the MI algorithm that is implemented in \pkg{mice}: `Multiple Imputation using Chained Equations' (MICE). This note addresses the question: 
%\emph{`How to diagnose convergence of the multiple imputation algorithm MICE?'.} 

The convergence properties of the MI algorithm in \pkg{mice} are investigated through model-based simulation. The results of this simulation study are guidelines for assessing convergence of MI algorithms. These guidelines will be implemented in an interactive evaluation tool for \pkg{mice}, `ShinyMICE', which is currently under development. All programming code used in this note is available from \href{https://github.com/gerkovink/shinyMice/simulation}{github.com/gerkovink/ShinyMICE/simulation}. 
%I will replicate Lacerda et al.'s simulation study on $\widehat{R}$ \citep{lace07}, and develop novel guidelines for assessing convergence. Ideally, I will integrate several diagnostics (e.g., $\widehat{R}$, \emph{autocorrelation}, and \emph{simulation error}) into a single summary indicator to flag non-convergence. 


%% -- Features ---------------------------------------------------------------
\subsection{Terminology} \label{sec:terms}

The intended audience of this note consists of empirical researchers and statisticians who use multiple imputation to solve missing data problems. Basic familiarity with MI methodology is assumed. For the theoretical foundation of MI, see \cite{rubin87}. For an accessible and comprehensive introduction to MI from an applied perspective, see \cite{buur18}.  

The convergence guidelines introduced in this paper are developed to be integrated into the \pkg{mice} environment \citep{mice} in \proglang{R} \citep{R}. This note, therefore, follows notation and conventions of \cite{mice}. Deviations from the `original' notation by \cite{rubin87} are described in \citep[\S~2.2.3]{buur18}. 

Let $Y$ denote an $n \times p$ matrix containing the data values on $p$ variables for all $n$ units in a sample. The collection of observed data values in $Y$ is denoted as $Y_{obs}$; the missing part of $Y$ is referred to as $Y_{mis}$. \textbf{REMOVE? Response indicator $R$ shows whether a data value in $Y$ is missing or observed.} The relation between $R$, $Y_{obs}$, and $Y_{mis}$ determines the missingness mechanism. This note only considers a `missing completely at random' (MCAR) mechanism, where the probability of being missing is equal for all $n \times p$ cells in $Y$.

In this note, the terms `unobserved' and `missing' data are used interchangeably to refer to $Y_{mis}$. The terms `incomplete' or `observed' data denote $Y_{obs}$. Incomplete data is the starting point of the multiple imputation procedure. Figure \ref{fig:steps} provides an overview of the steps involved with MI.

\begin{figure}
\label{fig:steps}
\centering
	\large{\begin{tikzpicture}
	\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 20mm]
\tikzstyle{connect}=[-latex, thick]
\node[main, fill = white!100] (data) [label=left:Incomplete data] { };
\node[main] (mids) [right=of data] { };
\node[main] (mids2) [above=10mm of mids] { };
\node[main] (mids3) [below=10mm of mids,label=below:Imputed data] { };
\node[main] (mira) [right=of mids] {};
\node[main] (mira2) [above=10mm of mira] { };
\node[main] (mira3) [below=10mm of mira,label=below:Analysis results] { };
\node[main, fill = white!100] (mipo) [right=of mira,label=right:Pooled result] { };
\path (data) edge [connect] (mids)
      (data) edge [connect] (mids2)
      (data) edge [connect] (mids3)
      (mids) edge [connect] (mira)
      (mids2) edge [connect] (mira2)
      (mids3) edge [connect] (mira3)
		  (mira) edge [connect] (mipo)
		  (mira2) edge [connect] (mipo)
		  (mira3) edge [connect] (mipo);
\end{tikzpicture}}
\caption{Scheme of the main steps in multiple imputation ($m = 3$). Adapted from \cite[\S~1.4.1]{buur18}.}
\end{figure}

Missing data in $Y$ is `imputed' (i.e., filled in) $m$ times. The imputed data is combined with observed data $Y_{obs}$ to create $m$ completed data sets. On each completed data set, the analysis of scientific interest (or `complete data model') is performed. The quantity of scientific interest (e.g., a regression coefficient) is denoted with $Q$. Since $Q$ is estimated on each completed data set, $m$ separate $\hat{Q}$ values are obtained. These $m$ values are combined into a single pooled estimate $\bar{Q}$.

This note focuses on the algorithmic properties of the imputation step. The algorithm employed within this step has an iterative nature. That is, before drawing $m$ imputed values for each missing data point in $Y_{mis}$, a `chain' of potential values is sampled. Each of the $m$ chains starts with an initial value, drawn randomly from $Y_{obs}$. The chains are terminated after a predefined number of iterations. Only the ultimate sample that a chain lands on is imputed and subsequently used in the analysis and pooling steps. The collection of samples across iterations (between the initial value and the imputed value) for each of the $m$ imputations will be referred to as an `imputation chain'. The total number of iterations per chain will be denoted with $T$, where $t$ varies over the integers $1, 2, \dots, T$.

% "Let Q denote the quantity of scientific interest (e.g., a regression coefficient)." \cite[p.~4]{mice}.
% "Let Y denote the n x p matrix containing the data values on p variables for all n units in the sample. We define the response indicator R as an n x p 0-1 matrix. The elements of Y and R are denoted by y i j and r i j , respectively, where i = 1 , ... , n and j = 1 , ... , p . If y i j is observed, then r i j = 1 , and if y i j is missing, then r i j = 0 ." ... The observed data are collectively denoted by Y o b s . The missing data are collectively denoted as Y m i s , and contain all elements y i j where r i j = 0 . When taken together Y = ( Y o b s , Y m i s ) contain the hypothetically complete data." \cite[par.~2.2]{buur18}
% The data are said to be MCAR if Pr ( R = 0 | Y o b s , Y m i s , $\psi$ ) = Pr ( R = 0 | $\psi$ ) (2.1) so the probability of being missing depends only on some parameters $\psi$ , the overall probability of being missing. The data are said to be MAR if Pr ( R = 0 | Y o b s , Y m i s , $\psi$ ) = Pr ( R = 0 | Y o b s , $\psi$ ) (2.2) so the missingness probability may depend on observed information, including any design factors. Finally, the data are MNAR if Pr ( R = 0 | Y o b s , Y m i s , $\psi$ ) (2.3) does not simplify, so here the probability to be missing also depends on unobserved information, including Y m i s itself. \cite[par.~2.2]{buur18}

% Also introduce: 
% 
% - Terminology (MCAR, MAR, MNAR)?
% 
% - Blue points are observed, the red points are imputed?

%% -- Theoretical Background ---------------------------------------------------------------
\subsection{Theoretical Background} \label{sec:background}

There is no scientific consensus on the convergence properties of multiple imputation algorithms \citep{taka17}. Some default techniques in \pkg{mice} might not yield converged states at all \citep{murr18}. Algorithmic convergence should, therefore, be monitored carefully. In that respect is the current evaluation practice insufficient. That is, the  recommendation is to visually inspect imputation chains for signs of non-convergence, which may be challenging to the untrained eye \citep[\S~6.5.2]{buur18}. Quantitative evaluation of the convergence of MI algorithms would be preferred.

The multiple imputation algorithm in \pkg{mice} is a special case of Markov chain Monte Carlo (MCMC) methods---a framework of iterative algorithmic procedures. Within this framework, several convergence diagnostics have been proposed, but the application of these measures on MI algorithms has not been systematically studied \citep{buur18}.
% For the framework of itrIt is not known whether convergence diagnostics that are available for the general framework that MICE is a special case of are appropriate for the evaluation of MI data. This is the framework of Markoc chain Monte Carlo %Yet, reference manuals of promiment MI software do mention the importance of evaluating algorithmic convergence (e.g., \cite[p.XYZ]{mice}, \textbf{XYZ, XYZ}). 
This section provides a brief review of:% briefly reviews?
1) how convergence is defined in the general context of iterative algorithmic procedures; 2) what diagnostic tools are available; and 3) which of these may apply to MI algorithms. 

Loosely speaking, MCMC algorithms aim to sample values from an unknown target distribution (e.g., the posterior distribution of a parameter in Gibbs samplers, or the distribution of missing values in MI algorithms). Convergence is reached once the algorithm samples exclusively from the target distribution \citep{cowl96}. Since this target distribution is unknown by definition, convergence can only be monitored by evaluating signs of non-convergence \citep{hoff09}. In practice, the definition of convergence consists of two components: `mixing' of chains together, and `stationarity' of individual chains \citep[p.~284]{gelm13}. The mixing component is satisfied when chains intermingle such that the only difference between the chains is caused by the randomness induced by the algorithm. Stationarity refers to the absence of trending across iterations within chains. %\textbf{Figure XYZ shows a pathological case of non-convergence concerning both components.}  
%The aim of MCMC procedures is to sample from an unknown (theoretical) distribution. The stability of the solution denpends on two components of convergence: should have converged, two components should be satisfied.

% Assessing convergence is difficult. We can never be certain of convergence, therefore convergence diagnostics evaluate signs of non-convergence \citep{hoff09}. % In general: "In general, you cannot know for sure if your chain has converged. But sometimes you can know if your chain has not converged, so we at least check for this latter possibility" \cite[p.~101]{hoff09}
% More importantly, in MCMC algorithms like MICE, "what is produced by the algorithm at convergence is not a single number or even a distribution, but rather a sample from a distribution" \citep[p.~883]{cowl96}. "automated convergence monitoring (as by a machine) is unsafe and should be avoided" \citep[p.~902]{cowl96}.
%Convergence has two components: `mixing' and `stationarity' \citep[p.~284]{gelm13}. Mixing refers to the variation between chains of samples. Stationarity has to do with stability of samples within chains. Ideally, both components will be satisfied: there are no trends within chains, and the chains intermingle such that the only difference between the chains is caused by the randomness induced by the algorithm. 

\textbf{[COULD BE SHORTER!]} Most convergence diagnostics target either of the two components. The stationarity component of convergence may be evaluated with autocorrelation \citep[$AC$; ][]{scha97, gelm13}, numeric standard error \citep[or `MC error'; ][]{gewe92}, and Raftery and Lewis's (\citeyear{raft91}) procedure to determine the effect of trending within chains. The mixing component can be assessed with the potential scale reduction factor $\widehat{R}$ \citep[a.k.a. `Gelman-Rubin statistic';][]{gelm92}. With an adapted version of $\widehat{R}$, proposed by \cite{veht19}, we might also evaluate the stationarity component of convergence. This would make $\widehat{R}$ a general convergence diagnostic. The application of $\widehat{R}$ to assess stationarity has not been thoroughly investigated. Therefore, this study employs both $\widehat{R}$ and autocorrelation to investigate convergence, as recommended by \cite[p.~898]{cowl96}.
%Therefore, the focus of this study is mainly on $\widehat{R}$. Stationarity will also be evaluated with autocorrelation as diagnostic, as recommended by \cite[p.~898]{cowl96}. 
Other convergence diagnostics are outside the scope of this study.\footnotemark 
\footnotetext{All of these methods evaluate the convergence of univariate scalar summaries (e.g., chain means or variances). These convergence diagnostics cannot diagnose convergence of multivariable statistics (i.e., relations between scalar summaries). \cite{buur18} proposed to implement multivariable evaluation through eigenvalue decomposition \cite{mack03}. This method is outside of the scope of the current study.}%  he eigenvector decomposition method proposed by McKay \citep{XYZ}.} These four methods, however, are outside of the scope of this study. Here, the focus is mainly on $\widehat{R}$ and autocorrelation.

%Diagnose non-convergence in the mixing sense: $\widehat{R}$. Diagnose non-convergence in the stability sense: autocorrelation (AC). Also, it is possible to compute MC errors, the Geweke statistic \textbf{\citep{XYZ}}, or to determine the appropriate number of iterations for convergence by employing \textbf{Raftery and Lewis's (1992)} method. All of these methods evaluate the convergence of univariate scalars of interest (e.g., chain mean or chain variance). The convergence of multivariable statistics (i.e. relations between such scalars) may be evaluated via the eigenvector decomposition method proposed by McKay \citep{XYZ}. These four methods, however, are outside of the scope of this study. Here, the focus is mainly on $\widehat{R}$ and autocorrelation.}

% "In order to converge to a stationary distribution, a Markov chain needs to satisfy three important properties (Roberts 1996; Tierney 1996): irreducible, the chain must be able to reach all interesting parts of the state space; aperiodic, the chain should not oscillate between different states; recurrence, all interesting parts can be reached infinitely often, at least from almost all starting points. Do these properties hold for the MICE algorithm? Irreducibility is generally not a problem since the user has large control over the interesting parts of the state space. This flexibility is actually the main rationale for FCS instead of a joint model. Periodicity is a potential problem, and can arise in the situation where imputation models are clearly inconsistent. A rather artificial example of an oscillatory behavior occurs when Y 1 is imputed by Y 2 $\beta$ + $\epsilon$  1 and Y 2 is imputed by - Y 1 $\beta$ + $\epsilon$  2 for some fixed, nonzero $\beta$ . The sampler will oscillate between two qualitatively different states, so the correlation between Y 1 and Y 2 after imputing Y 1 will differ from that after imputing Y 2 . In general, we would like the statistical inferences to be independent of the stopping point. A way to diagnose the ping-pong problem, or order effect, is to stop the chain at different points. The stopping point should not affect the statistical inferences. The addition of noise to create imputations is a safeguard against periodicity, and allows the sampler to "break out" more easily. Non-recurrence may also be a potential difficulty, manifesting itself as explosive or non-stationary behavior. For example, if imputations are made by deterministic functions, the Markov chain may lock up. Such cases can sometimes be diagnosed from the trace lines of the sampler. See Section 6.5.2 for an example. As long as the parameters of imputation models are estimated from the data, non-recurrence is mild or absent. The required properties of the MCMC method can be translated into conditions on the eigenvalues of the matrix of transition probabilities (MacKay 2003, 372-73). The development of practical tools that put these conditions to work for multiple imputation is still an ongoing research problem." \cite[par.~4.5]{buur18}

The convergence diagnostics under consideration are $\widehat{R}$ and autocorrelation. Perfect convergence is reached when the variance between imputation chains is equivalent to the variance within chains ($\widehat{R} = 1$), and there is no dependency between subsequent samples within imputation chains ($AC = 0$). 
%
%%%%%%%%%%%%%%%%%
%As potential scale reduction factor, $\widehat{R}$ tells us by how much the variance of an estimate could be shrunken down if the chains were to be infinitely long \citep[$t \to \infty$;][]{gelm92}. If there is no added value compared to infinitely many iterations, the chains have converged. 
%%%%%%%%%%%%%%%%%%
%
%$\textbf{Explain the measures that I'm investigating here. R hat is ... autocorrelation is... How are they computed? What do they say? How are they computed on MI data? Looking at imputation chains. 
%Explain that there is no baseline to compare the measures with. The only current convergence check is to visually inspect trace plots. (But in the trace-plots convergence seems immediate).}
%
%The potential scale reduction factor $\widehat{R}$ tells us by how much the variance of an estimate could be shrunken down if the chains were to be infinitely long \cite{gelm92}. That then informs us about how dependent the chains are on the starting values. If there is no dependence on the initial values anymore, the chains have converged (in the mixing sense of the word). 
%$\widehat{R}$ is then equal to one. 
%
%We can apply $\widehat{R}$ to the mean (or to the first two moments) of the variables of interest. 
To define $\widehat{R}$, we follow notation by \citep[p.~5]{veht19}. % The scalar summary of interest in this study is the average of the imputed values per variable. %As recommended by \cite{veht19} $\widehat{R}$ is computed as .... to be able to detect ... in the tails of the distribution. \cite[p.~5]{veth19}
Let $M$ be the number of chains, $N$ the number of iterations per chain, and $\theta$ the scalar summary of interest (e.g., chain mean or chain variance).
We compute the variance within each chain of $\theta$ values ($W$) and between the $m$ chains ($B$) as follows:
%\textbf{REPHRASE: In the equations below, $N$ is the number of draws per chain, $M$ is the number of chains, and $S = MN$ is the total number of draws from all chains. For each scalar summary of interest $\theta$, we compute $B$ and $W$, the between- and within-chain variances:}

\begin{align*}
B&=\frac{N}{M-1} \sum_{m=1}^{M}\left(\bar{\theta}^{(\cdot m)}-\bar{\theta}^{(\cdot \cdot)}\right)^{2}, \text { where } \bar{\theta}^{(\cdot m)}=\frac{1}{N} \sum_{n=1}^{N} \theta^{(n m)}, \quad \bar{\theta}^{(\cdot \cdot)}=\frac{1}{M} \sum_{m=1}^{M} \bar{\theta}^{(\cdot m)} \\
W&=\frac{1}{M} \sum_{m=1}^{M} s_{j}^{2},  \text { where } s_{m}^{2}=\frac{1}{N-1} \sum_{n=1}^{N}\left(\theta^{(n m)}-\bar{\theta}^{(\cdot m)}\right)^{2}. %\text{ \cite[p.~5]{veht19}} 
\end{align*}

From the between and within chain variances we compute a weighted average, $\widehat{\operatorname{var}}^{+}$, which over-estimates the total variance of $\theta$. $\widehat{R}$ is then obtained as a ratio between the over-estimated total variance and the within chain variance:

\begin{equation*}
\widehat{R}=\sqrt{\frac{\widehat{\operatorname{var}}^{+}(\theta | y)}{W}},
\text{ where } \widehat{\operatorname{var}}^{+}(\theta | y)=\frac{N-1}{N} W+\frac{1}{N} B.
\end{equation*}


%The proportion of within chain variance $W$ against the weighted average of $B$ and $W$, $\widehat{\operatorname{var}}^{+}$ tells us how much variance of $\theta$ could be shrunken down if $N\to\infty$. $\widehat{\operatorname{var}}^{+}$ is computed as follows: %over-estimates the marginal posterior variance of the estimand, $\theta$. 

% \begin{equation*}
% 
% \end{equation*}

%The potential scale reduction factor $\widehat{R}$ is obtained as:
We can interpret $\widehat{R}$ as potential scale reduction factor since it indicates by how much the variance of $\theta$ could be shrunken down if an infinite number of iterations per chain would be run \citep[$t \to \infty$;][]{gelm92}. High $\widehat{R}$-values, therefore, indicate slow mixing of chains, e.g., because of dependence across chains on initial values. Once there is no dependence on initial values anymore, there is no added value of continuing iterations. \textbf{[MOGELIJK OVERBODIGE ZIN HIER]} The measure then diagnoses algorithmic convergence ($\widehat{R} = 1$). 
%If there is no added value compared to infinitely many iterations, the chains have converged. 
The conventionally acceptable threshold was $\widehat{R} < 1.2$ \cite{gelm92}. Recently, \cite{veht19} proposed a more stringent threshold: $\widehat{R} < 1.01$. 

Following the same notation, autocorrelation is defined as the correlation between two subsequent $\theta$ values within the same chain. We only consider $AC$ at lag 1, i.e., between the $t^{th}$ and $t+1^{th}$ iteration of each chain: 
%
\begin{equation*}
AC = \left( \frac{T}{T-1} \right) \frac{\sum_{t=1}^{T-1}(\theta_t - \bar{\theta}^{(\cdot m)})(\theta_{t+1} - \bar{\theta}^{(\cdot m)})}{\sum_{t=1}^{T}(\theta_t - \bar{\theta}^{(\cdot m)})^2},
\end{equation*}
%
where $T$ is the total number of iterations per chain \citep[p.~147]{lync07}. We can interpret the sign of $AC$-values as a measure of dependence within chains ($-1 \leq AC \leq 1$). Positive $AC$-values indicate recurrence: similar $\theta$-values for subsequent iterations, which may lead to trending. Negative $AC$-values also indicate dependence, but do not threaten the stationarity component of convergence \textbf{[REMOVE INTERPRETATION HERE?]}. Instead of recurrence, there would be a `seesaw' effect within chains: subsequent $\theta$-values would deviate in opposite directions from the chain average $\theta^{(\cdot m)})$, increasing the variance of $\theta$. As convergence diagnostic, the interest is in positive $AC$-values.\footnotemark 
\footnotetext{The magnitude of $AC$-values can also be evaluated statistically, but that is outside of this note's scope.}

%, where $\theta$ is a scalar summary of interest (i.e., chain means and chain variances).
%Non-recurrence can be evaluated with autocorrelation. autocorrelation shows how dependent subsequent draws of an imputation chain are on the previous value. If there is a lot of dependence, draws at e.g. iteration five are significantly correlated with the value of the first draw. 
%A high autocorrelation indicates dependence within chains. The magnitude of the AC can be interpreted qualitatively or quantitatively. Quantitative evaluation of the AC entails comparing the observed ACs to the critical values of a two-tailed 95\% confidence interval, divided by the square root of the number of iterations. %. critical value corresponding to the $z-$~value standard normal distribution with \textbf{XYZ} degrees of freedom.
%  AC_lim <- qnorm(.975) / sqrt(maxit)

%\textbf{HIER VALT TE SCHRAPPEN!} It is not known whether $\widehat{R}$ and $AC$ are appropriate measures to diagnose convergence of multiple imputation algorithms. 


% the conventional threshold $\widehat{R} < 1.1$ might be too stringent. \cite[pp.~49-50]{lace07} report
% 
% $\widehat{R}$ is sensitive enough to indicate non-convergence can  reported $\widehat{R}$-values  
% 
% An empirical finding, however, shows that the opposite may be true: \cite{lace07} report $\widehat{R}$-values above the threshold of $\widehat{R} < 1.1$ after fifty iterations. %that $\widehat{R}$ will not be smaller than 1.1 before iteration number 50. Therefore, the aim is to replicate empirical finding Lacerda et al. 
% 


%Conventional thresholds to diagnose non-convergence-- e.g., Gelman and Rubin's \citeyear{gelm92} statistic $\widehat{R} < 1.1$ --are not applicable on multiply imputed data \citep{lace07} \textbf{[Expand or remove Rhat here! Refer to future section?]}. Therefore, empirical researchers have to rely on visual inspection procedures that are theoretically equivalent to $\widehat{R}$ \citep{whit11} \textbf{[explain why this can still be appropriate, and numerical is not]}. Visually assessing convergence is not only challenging to the untrained eye, it might also be futile. 

% This research project consists of an investigation into algorithmic convergence of MI algorithms. I will replicate Lacerda et al.'s simulation study on $\widehat{R}$ \citep{lace07}, and develop novel guidelines for assessing convergence. Ideally, I will integrate several diagnostics (e.g., $\widehat{R}$, \emph{autocorrelation}, and \emph{simulation error}) into a single summary indicator to flag non-convergence.

\subsection{Simulation Hypothesis} \label{sec:hypothesis}

This study evaluates whether $\widehat{R}$ and $AC$ could diagnose convergence of multiple imputation algorithms. There is, however, no baseline measure available to evaluate their performance against.\footnote{Only severely pathological cases of non-convergence stand out when visually inspecting the imputation chains.} We will assess the appropriateness of the two convergence diagnostics by comparison with simulation quantities. The perfrormance diagnostics are the evaluation criteria  recommended by \citep[\S~2.5.2]{buur18}, whcih comprise of average bias, average confidence interval width, and empirical coverage rate across simulations.% %"We expect that in accordance with Neyman (1934), at least 95 percent of the confidence intervals should contain the true population value, although some room for simulation error should be taken into account to counteract the finite nature of our simulations"\cite{schou18}.
\footnote{We could also look at distributional characteristics, and plausibility of imputed values, see \cite{vinknd} (n.d.). For now, this is outside of the scope of this study.} % Or use: Then evaluate the regular diagnostics of MI simulations, see Vink, n.d., and  convergence. Diagnostics include absolute bias of the estimated regression coefficient, confidence interval width, and empirical coverage rate across simulations. Convergence is evaluated with $\widehat{R}$, or potential scale reduction factor.


\textbf{We hypothesize that $\widehat{R}$ will over-estimate non-convergence. No hypothesis was formulated about AC, results are exploratory.
%
[MOVE THIS TO DISCUSSION?] Assessing the stationarity component of convergence with $AC$ might be redundant, since high $AC$-values are implausible in MI procedures. That is, the randomness induced by the MI algorithm effectively mitigates the risk of dependency within chains. $AC$ would thus not be informative of the convergence of MI algorithms. 
%
Moreover, we hypothesize that $\widehat{R}$ may not be an appropriate diagnostic either, because it assumes over-dispersed initial values (i.e., the starting points of the chains are `far away' from the target distribution and each other). In \pkg{mice}, initial values of the algorithm are chosen randomly from the observed data. Therefore, we cannot be certain that the initial values are over-dispersed. %The initial parameter value is noodzakelijk. Daarom is het lastig om in MI that starten vanuit overdisp state. 
% Starting values of MICE chains: "For each j, fill in starting imputations $Y^0_j$ by random draws from $Y^{obs}_j$." \cite[\S~4.5.2]{buur18} before for loops over variables and imputations, see \url{https://stefvanbuuren.name/fimd/sec-FCS.html#def:mice}
Without over-dispersed initial states, $\widehat{R}$ may falsely diagnose convergence \citep{broo98}. %would not be sensitive enough to flag non-convergence of MI algorithms: "if over-dispersion does not hold, $\sigma_+^2$ can be too low, which can lead to falsely diagnosing convergence." \cite[p~437]{broo98}. 
This suggests that $\widehat{R}$ would not be sensitive enough to flag non-convergence of MI algorithms. However, an empirical finding by \cite{lace07} shows that $\widehat{R}$ can indicate non-convergence at unconventionally long chain lengths. While the default number of iterations in \pkg{mice} is five, $\widehat{R} > 1.1$ was observed after 50 iterations, and $\widehat{R} > 1.01$ up-to 150 iterations. This suggests that $\widehat{R}$ would over-estimate non-convergence of MI algorithms. }


%Convergence diagnostics for MI methods have not been systematically studied \citep{buur18}.  Moreover, 
%A fundamental assumption of MICE is convergence of the algorithm. MICE is a type of Markov chain Monte Carlo (MCMC) algorithm. In general, the validity of inference resulting from MCMC algorithms is threatened by non-convergence. Hence we use convergence diagnostics to flag non-convergence. But it is not known whether conventional convergence diagnostics for MCMC methods work on MI data.
%the need for convergence diagnostics is extra pressing in the context of the MICE algorithm.

%The primary research interest is in determining whether $\widehat{R}$ is an appropriate convergence diagnostic, and if so, which level of stringency suits MI data. \textbf{Or if there is a discrepancy between the visual and numeric inspection. There is no baseline to compare the convergence diagnostics with. Therefore, we will only evaluate performance measures.}

%This simulation study hypothesizes that $\widehat{R}$ will over-estimate non-convergence. %\textbf{[Or use:] Hypothesis based on \cite{lace07} is that the conventional acceptable level of $\widehat{R}$ is too strict for MI data. We expect that the simulation diagnostics will indicate valid inference before $\widehat{R}$ will.}



% Also introduce:
% 
% - missingness mechanims, ignorability?
% % "The practical importance of the distinction between MCAR, MAR and MNAR is that it clarifies the conditions under which we can accurately estimate the scientifically interesting parameters without the need to know $\psi$" \cite[par.~2.2]{buur18}.
% 
% - Rubin's rules?
% 
% - FCS vs. JM?
% 

% \subsubsection{What is already implemented?}
% 
% - Trace-plots
% 
% \subsubsection{What is not yet implemented, but exists?}
% 
% - $\widehat{R}$, but too stringent (new) threshold, and assumption of over-dispersed initial values of imputation chains not met.
% % Potentially add something about updated (2019) version of $\widehat{R}$ and the new threshold of 1.01, see \citep{veht19}.
% 
% - autocorrelation. Schafer (1997, p. 129) wrote on worst linear statistic. We could calculate the autocorrelation of that statistic to know that the algorithm converged elsewhere too. See autocorr function plot in SAS of worst linear function.
% % Worst linear function in SAS see \url{https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mi_sect027.htm}.
% % Note: we're talking about missing data only, not the combined data (that autocorrelation is very high, as is the autocorrelation of deductively imputed values, like in the texp example). 
% % "Applications of MICE with lowly correlated data therefore inject a lot of noise into the system. Hence, the autocorrelation over t will be low, and convergence will be rapid, and in fact immediate if all variables are independent. Thus, the incorporation of noise into the imputed data has pleasant side-effect of speeding up convergence" \citep{buur18}, par. 4.5. 
% 
% - Sensitivity analysis: Run algorithm several times and compare results. 
% % "Imputers who do choose to use FCS should use flexible univariate models wherever possible and take care to assess apparent convergence of the algorithm, for example by computing traces of pooled estimates or other statistics and using standard MCMC diagnostics (Gelman et al., 2013, Chapter 11). It may also be helpful to examine the results of many independent runs of the algorithm with different initializations and to use random scans over the p variables to try to identify any convergence issues and mitigate possible order dependence" \cite[p.~19]{murr18}.
% 
% \subsubsection{What is not implemented, and NA?}
% 
% - $\widehat{R}$ threshold: Replicate simulation study and build a decision rule to solve the problem with $\widehat{R}$.
% % "The monitoring statistic was computed for mean monthly earnings at each iteration in chains of length k = 200. Since calculation of the statistic requires M parallel sequences, m = 5 such chains were constructed. This value of m was informed by the preferred choice given in the literature on multiple imputation. The monitoring statistic computed at each iteration is presented in Figures 15 to 17 for each of the three missingness mechanisms. The red vertical line denotes ten iterations" \cite[p.~49]{lace07}.
% 
% - Stability of the solution: Possibly use the slope of means over iterations too to see whether there is trending. Or apply PCA on the imputed data and if that (the eigenvalues?) stays the same we know that the means and variances are stable as well, see McKay (?). 
% 
% - MC error: MC error = SD/sqrt(number of iterations), where SD represents the variation across iterations. The MC error thus represents how much the means differ w.r.t. the iterations. MC error decreases as number of iterations increases. It should not be larger than 5\% of the sample standard deviation.

%% -- Methods ---------------------------------------------------------------

%%
%%
%%
%%

\section{Methods} \label{sec:methods}

%To investigate the convergence properties of the multiple imputation algorithm in \pkg{mice}, w
We use model-based simulation in \proglang{R} to perform multiple imputation under 100 simulation conditions. In each condition, the MI algorithm comprises a different imputation chain length ($T = 1, 2, \dots, 100$). The number of simulation runs per condition is 1000. For each simulation condition (i.e., number of iterations) and each repetition (i.e., simulation run) we compute convergence diagnostics ($\widehat{R}$ and autocorrelation) and simulation diagnostics (bias, confidence interval width and coverage). These diagnostics are aggregated across repetitions to obtain averages per simulation condition.
%We compute $\widehat{R}$ and autocorrelation (`convergence diagnostics') and performance measures (`simulation diagnostics') for each condition. %All simulation conditions are replicated 1000 times. %For each of the 100 conditions 1000 simulation runs are performed. for each of the 100 simulation conditions. \textbf{Estimands are convergence diagnostics. Performance measures are simulation diagnostics.  where for one through one hundred the Rhat and ... are calculated.} 
The simulation set-up consists of several steps, summarized in the pseudo-code below.\footnote{The complete \proglang{R} script of the simulation study is available on Github.}

\begin{Code}
# pseudo-code of simulation 
simulate data 
for (number of simulation runs from 1 to 1000)
  for (number of iterations from 1 to 100)
    create missingness
    impute the missingness
    compute convergence diagnostics
    perform analysis
    pool results
    compute simulation diagnostics
aggregate convergence and simulation diagnostics
\end{Code}

\subsection{Data simulation}
The point of origin for all simulation conditions is a finite population of $N=1000$. The data are simulated to solve a multiple linear regression problem, where dependent variable $Y$ is regressed on independent variable $X$, and covariates $Z_1$ and $Z_2$: $Y \sim \beta_1 X + \beta_2 Z_1 + \beta_3 Z_2$. The quantity of scientific interest is regression coefficient $\beta_1$. The data generating model is a multivariate normal distribution with means structure $\mu$ and variance-covariance matrix $\Sigma$. 

\begin{align*}
\begin{pmatrix}X\\
Z_{1}\\
Z_{2}\\
\epsilon
\end{pmatrix} \sim  N
\begin{bmatrix}
\begin{pmatrix}
12\\
3\\
0.5\\
0
\end{pmatrix}\!\!,
\begin{pmatrix}
4 & 4 & 1.8 & 0\\
4 & 16 & 4.8 & 0\\
1.8 & 4.8 & 9 & 0\\
0 & 0 & 0 & 100
\end{pmatrix}
\end{bmatrix}\\[2\jot]
\end{align*}

Outcome variable $Y$ is subsequently calculated as $Y =  2X + .5Z_1 - Z_2 + \epsilon$.

\subsection{Amputation}
The complete dataset is `amputed' once in each simulation repetition. That is, the \pkg{mice} function \fct{ampute} is used to impose a missingness mechanism upon the data. The missingness is univariate, and the probability to be missing is the same for all variables, namely 20\% (\texttt{prop = 0.8, mech = "MCAR"}). This leaves 20\% of the rows completely observed. %The resulting amputed data is equal for all simulation conditions in the same repetition. %"Furthermore, we generate three kinds of missingness proportions: 0.1, 0.5, and 0.9. Note that these values indicate the sampled proportion of incomplete cases in Y" \cite{schou18}.

\subsection{Imputation}
Missing data points are imputed with the \pkg{mice} function \fct{mice}. All simulations are performed with Bayesian linear regression imputation (\texttt{method = "norm"}), and five imputation chains (\texttt{m = 5}). The number of iterations varies between simulation conditions (\texttt{maxit = 1, 2, \dots, 100}). 
% Missingness mechanism: "With MCAR missingness mechanisms, the probability to be missing is the same for all cases. This is a necessary simulation condition for evaluating the performance of imputation procedures. If an imputation method is not able to solve the problem (i.e. yield valid inference) under MCAR, the statistical properties of the procedure are not sound." (Vink, n.d., p. 4)
% "combine the m completed data inferences into a single inference following Rubin's (1987) rules (pp. 76-77)" \cite{schou18}.

\subsection{Convergence Diagnostics}
% Add ``post-processing steps (e.g. aggregation computations, summary statistics, regressions on the simulation results) used to transform simulation outputs to reported results''. %, see \url{http://jsterman.scripts.mit.edu/docs/Rahmandad-Sterman%20Simulation%20Reporting%20Standards3.30%20update.pdf}.
We investigate convergence by extracting not only the imputed values of each MI procedure (the $T^{th}$ iteration), but also information ($\theta$-values) from the intermediate iterations ($t = 1, 2, \dots, T$). The $\theta$s of interest are chain means and chain variances per variable. We compute convergence diagnostics separately for each of the four variables in the dataset, and report the maximum (absolute) value as the `worst linear predictor' of convergence \textbf{[Rephrase or footnote??]}. $\widehat{R}$ is computed by implementing the definition given in \textbf{EQUATION NR}. $AC$ is computed with \pkg{stats} function \fct{acf}.

% 
% 
% extract chain means and chain variances per iteration $t$. From the MI algorithm Convergence diagnostics are computed for chain means and chain variances.   
% $\widehat{R}$ is computed within imputation chains: . The maximum value across variables within the same simulation is reported. 
% 
% autocorrelation (AC) is computed within imputation chains,  The AC of the variable with the highest absolute AC value is reported per simulation. 


\subsection{Analysis}

To estimate the quantity of scientific interest, $Q$, we perform multiple linear regression on each completed dataset with the \pkg{stats} function \fct{lm}. We obtain an estimated regression coefficient per imputation, which are pooled into a single estimate, $\bar{Q}$. We use the \pkg{mice} function \fct{pool} to get variance estimates according to Rubin's \citeyear{rubin87} rules, and utilize these to implement finite population pooling as per \citep{vink14}.

\subsection{Simulation diagnostics}
% Distributional characteristics: In practice, the distribution of the incomplete data may differ greatly from the observed data. Under anything but the MCAR assumption, this can be expected. When evaluating imputations, the distributional shapes should be checked and diagnostic evaluations should be performed (see Abayomi et al., 2008, for an detailed overview of diagnostic evaluation for multivariate imputations). When anomalies are found, and if the imputation method is valid, there should be an explanation, especially in the controlled environment of a properly executed simulation study. 
% Plausibility of the imputed values: Plausible imputations - imputations that could be real values if they had been observed - are not a necessary condition for obtaining valid inference. However, in practice, especially when the imputer and the analyst are different persons, plausibility of imputations may be a desired property. When evaluating imputation routines, the evaluator should mention whether the routine is prone to deliver implausible value

We compute bias as the difference between $\bar{Q}$ and $Q$. Confidence interval width (CIW) is defined as the difference between the lower and upper bound of the 95\% confidence interval around $\bar{Q}$ (CI95\%). We compute the CI95\% bounds as $\bar{Q} \pm 2.66 \times SE_{\bar{Q}}$, where 2.66 is the quantile of a $t$-distribution with $m-1$ degrees of freedom, and $SE_{\bar{Q}}$ is the square root of the pooled variance estimate. From bias and CIW, we calculate empirical coverage rates. Coverage rate is the proportion of simulations in which $Q$ is between the bounds of the CI95\% around $\bar{Q}$. 

%% -- Results ---------------------------------------------------------------

%%
%%
%%
%%

\section{Results}


Figures \ref{fig:conv} and \ref{fig:sim} display results per simulation condition ($T = 1,2,\dots,100$). A subset of simulation conditions is presented in Table \ref{tab:results}. 

\begin{figure}[h]
  \resizebox{\textwidth}{!}{ %notice the \resizebox{} command
        \includegraphics{Figures/convergence_diag.pdf}
  }
  \caption{Convergence diagnostics over 1000 MCMC simulations.}
    \label{fig:conv}
\end{figure}

\begin{figure}[h]
  \resizebox{\textwidth}{!}{ %notice the \resizebox{} command
        \includegraphics{Figures/simulation_diag.pdf}
  }
  \caption{Simulation diagnostics over 1000 MCMC simulations.}
    \label{fig:sim}
\end{figure}

% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Wed Dec 04 18:16:11 2019
\begin{table}[ht]
\centering
\caption{Simulation and convergence diagnostics over 1000 MCMC simulations.} 
\label{tab:results}
\begin{tabular}{lrrrrrrr}
  \hline
T & Bias & CI width & Cov. rate & $\widehat{R}_{mean}$ & $\widehat{R}_{var}$ & $AC_{mean}$ & $AC_{var}$ \\ 
  \hline
   1 & -0.137 & 0.954 & 0.932 & NA & NA & NA & NA \\ 
     2 & -0.006 & 0.932 & 0.953 & 1.650 & 1.632 & -0.500 & -0.500 \\ 
     3 & 0.002 & 0.929 & 0.944 & 1.314 & 1.306 & -0.660 & -0.659 \\ 
     4 & 0.003 & 0.933 & 0.957 & 1.461 & 1.457 & -0.733 & -0.735 \\ 
     5 & 0.004 & 0.935 & 0.954 & 1.475 & 1.472 & -0.705 & -0.706 \\ 
     6 & 0.001 & 0.934 & 0.956 & 1.258 & 1.256 & -0.656 & -0.646 \\ 
     7 & 0.002 & 0.930 & 0.948 & 1.265 & 1.269 & -0.591 & -0.585 \\ 
     8 & 0.004 & 0.930 & 0.954 & 1.181 & 1.178 & -0.495 & -0.516 \\ 
     9 & 0.003 & 0.931 & 0.956 & 1.187 & 1.187 & -0.442 & -0.459 \\ 
    10 & 0.003 & 0.952 & 0.943 & 1.141 & 1.140 & -0.403 & -0.423 \\ 
    15 & 0.002 & 0.942 & 0.965 & 1.100 & 1.100 & -0.276 & -0.289 \\ 
    25 & 0.005 & 0.934 & 0.955 & 1.057 & 1.057 & -0.143 & -0.159 \\ 
    50 & -0.002 & 0.929 & 0.959 & 1.027 & 1.026 & -0.053 & -0.075 \\ 
   100 & 0.000 & 0.920 & 0.946 & 1.013 & 1.013 & 0.022 & -0.017 \\ 
   \hline
\end{tabular}
\end{table}


%% --------------------------------
\subsection{Convergence diagnostics}
It is apparant that there is a relation between convergence diagnostics $\widehat{R}$ and $AC$, and the number of iterations per simulation condition($T$). Generally speaking, we see a trend towards convergence as $T$ increases with each simulation condition ($\widehat{R}$-values approache one, and $AC$-values approache zero). The convergence diagnostics show more or less equivalent trends for chain means versus chain variances. We, therefore, only discuss the $\widehat{R}$ and $AC$-values of chain means. %convergence diagnostics for the chain means. %do not differentiate in the interpretation of chain means . 

Figure \ref{fig:conv}A shows that $\widehat{R}$-values generally decrease with increasing imputation chain lengths. The decline stabilizes somewhere between the simulation conditions $T=30$ and $T=50$. The downward trend is most pronounced for $T=3$, and between $T = 5$ and $T = 10$. In the intervening conditions ($3 \leq T \leq 5$), however, we observe a steep increase in $\widehat{R}$-values. This increase implies that convergence cannot be diagnosed at $T=3$, as would be the case based on the conventional threshold $\widehat{R} < 1.2$. According to the widely used threshold $\widehat{R} < 1.1$, convergence can be diagnosed for conditions where $T>9$. If we use the recently recommended threshold $\widehat{R} < 1.01$, we would conclude that we cannot diagnose convergence in any simulation condition.

%The $\widehat{R}$-values for chain means and chain variances show equivalent trends, neither is there a clear difference between the four variables. Roughly speaking, we see that $\widehat{R}$ is higher for simulation conditions with a lower number of iterations, and approaches one for conditions with more iterations. 

%%%%
%Figure \ref{fig:conv} shows an initial dip, a small bump upwards, and then a steep decline. This decline continues up-to iteration twenty, after which a more gradual decrease can be observed up-to iteration forty. The $\widehat{R}$-values are more or less stable after that. 
%\textbf{[Remove stuff here]} %The conventionally acceptable threshold for convergence, maximum 
%$\widehat{R} < 1.2$ is reached at $T =3$, and conditions with six iterations or more. The widely used threshold $\widehat{R} < 1.1$ is met in conditions with ten or more iterations. The most recent recommended threshold is not reached within the 100 iterations considered in this simulation study. $\widehat{R} > 1.01$ for all simulation conditions (see online appendix for the full table of results).
\textbf{say something about Rh below 1.}
%%%%

The $AC$-values displayed in Figure \ref{fig:conv}B are almost uniformly increasing as a function of $T$. The only $AC$-value that deviates from this observation is for $T=2$. The lowest $AC$-value is obtained for $T=3$. At $T=5$, we reach the same $AC$-value as observed at $T=2$. $AC$-values plateau around $T=70$. $AC$ is then indifferentiable from zero, indicating stationarity. %Simulation conditions where $T>5$ have $AC$-values greater than $T=2$. %We see an initial decrease between $T=2$ and $T=3$.
According to this diagnostic, none of the simulation conditions show signs of non-convergence, since we only observe negative or zero $AC$-values.

% All observed autocorrelations  are negative or close to zero. There are some differences between variables, and the ACs of chain variances seem lower than the ACs of chain means. The general trend in figure \ref{fig:conv} is that ACs decrease steeply as the number of iterations increases.  \textbf{The lowest AC observed is %-0.735 
% at four iterations. Simulation conditions with a higher number of iterations show a slow increase towards zero. autocorrelations do not plateau completely within the range of simulation conditions considered in this study. It seems that ACs might become positive if more iterations would be allowed. The average absolute maximum ACs are negative up to 77 iterations for chain means, and for chain variances all iterations considered here have negative ACs.}

% \textbf{[Add simulation time (computational cost) to table? Add empirical SE of all diagnostics? Sample some random simulations and check distribution and plausibility of imputations? Use chain variance as scalar of interest for rhat and autocorrelation?]}

%% --------------------------------
\subsection{Simulation diagnostics}
\textbf{Bias.} Figure \ref{fig:sim} shows the average bias between the regression coefficient of $X$ on $Y$ in the complete dataset, and $\bar{Q}$, \textbf{the pooled estimated regression coefficient and the ... Average bias is close to zero for simulation conditions with two iterations or more. The simulation condition with one iteration has a negative bias. From two iterations upwards, the average bias across repetitions is stable. Across iterations two to one hundred, bias fluctuates within a narrow range around zero. Loess line is flat around it = 20.} 
% Average bias is not zero because there is only one incomplete dataset to be imputed. Bias is a sample effect.

\textbf{Confidence interval width.} CIW is only clearly divergent for the simulation condition with one iteration. Conditions with two or more iterations have similar CIWs. The magnitude of differences from the loess line is somewhat larger up-to the condition with four iterations. From four iterations on, CIW seems to be stable across iterations. \textbf{Loess line is never flat.}%One might argue that there is some downward trending up-to thirthy to forty iterations. %After about twenty-five iterations the average CIW across all simulation conditions and repetitions is reached 0.928. 

\textbf{Coverage rate.} \textbf{The coverage rate is more or less stable from two iterations upwards. On average, the coverage rate is somewhat higher than the expected nominal coverage of 95\% \cite{neym34}, namely 95.3\%. Loess line is never flat.}

\textbf{give some implications for these measures. connect convergence and simulation diagnostics.}

%% -- Summary/conclusions/discussion -------------------------------------------
%%
%%
%%
%%

\section{Summary and discussion} \label{sec:summary}

\textbf{Answer RQ.} How to diagnose non-convergence? It's complicated.

\textbf{Summary.}
This note illustrates that conventional convergence diagnostics behave differently on MI data than other MCMC methods like Gibbs samplers in Bayesian analyses. The most recent recommended threshold for $\widehat{R}$ ($\widehat{R} < 1.01$) may be too stringent for MI data. \textbf{However, using the 1.2 threshold may be too lenient because this was observed at three iterations and then again after six. If we would use 1.2, we might miss this increase after three iterations. The 1.1 threshold seems OK. Also, ten iterations are computationally not terrible. With increased performance and storage capacities, doubling the default is fine I guess.}

From the simulation diagnostics, it appears that as little as three iterations might be sufficient to obtain unbiased, confidence valid estimates. Convergence diagnostics $\widehat{R}$ and autocorrelation, however, indicate that convergence may only be reached after twenty or even forty iterations. 

\textbf{R hat below 1.}
$\widehat{R}$ could theoretically not be smaller than one, yet it occurred several times in this study (see online appendix XYZ). This can happen when the number of simulations is smaller than in `regular' MCMC processes [explain that fewer iterations is an advantage of MI, not a disadvantage compared to MCMC]. Increasing Rhat values mean that the initial values were not appropriately over-dispersed \citep[p~438]{broo98}. %Therefore, the `$(n-1/n)$' [add equation number] correction factor can influence the estimated potential scale reduction factor. This downwards bias is in the opposite direction than expected: ``The mixture-of-sequences variance, $V$, should stabilize as a function of $n$. (Before convergence, we expect $\sigma^2$ to decrease with n, only increasing if the sequences explore a new area of parameter space, which would imply that the original sequences were not overdispersed for the particular scalar summary being monitored.)'' \cite[p~438]{broo98}.

% \url{https://discourse.mc-stan.org/t/rhat-1-as-low-as-9-94e-01-why/9252/3}

\textbf{Negative ACs.} autocorrelation is dangerous when positive. These autocorrelations are negative. Still, we want the iterations to be stable and independent. Default maxit is five iterations now, should this be different? Are the 'waves' most pronounced at iteration 5? But why the dip??  

The observed dip in AC implies that default maxit value of five iterations is the worst possible number of iterations.

\textbf{Future research.} This study considered only an MCAR missingness mechanism. Necessary but not sufficient to demonstrate the performance of convergence diagnostics. This is just a proof of concept. Further research is needed to investigate the performance under violation of convergence, e.g. dependency between predictors (very high correlations). %We would like to have convergence measures for multivariable statistics (scalars?) of interest. This is, however, dependent on the complete data model. The eigenvector decomposition method proposed by McKay (?) should be implemented. I could not find any resources to apply this method and it is outside the scope of this thesis to investigate how this approach could be implemented. \textbf{[this paragraph is too negative, explain scope in intro instead.]}

%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

The results in this paper were obtained using \proglang{R}~3.6.1 \cite{R} with the \pkg{mice}~3.6.0.9000 package \cite{mice}. \proglang{R} itself and all packages used are available from the Comprehensive \proglang{R} Archive Network (CRAN) at \url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}

This note is part of a Research Master's thesis project. It was written by the sole author (Hanne Oberman, BSc.), with guidance from Master thesis supervisors prof. dr. Stef van Buuren and dr. Gerko Vink, and research seminar mentor dr. Daniela Cianci. 


%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{ShinyMICE}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

\begin{appendix}

\end{appendix}

%% -----------------------------------------------------------------------------


\end{document}

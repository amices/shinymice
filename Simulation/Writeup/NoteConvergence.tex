\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package (only for this demo article)
% \usepackage{framed}
\usepackage{multirow, amsmath}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}



%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Hanne Oberman\\Utrecht University}
   % \And Second Author\\Plus Affiliation}
\Plainauthor{Hanne Oberman}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{A Note on Convergence Diagnostics for Multiple Imputation using Chained Equations (MICE)}
\Plaintitle{A Note on Convergence Diagnostics for Multiple Imputation using Chained Equations (MICE)}
\Shorttitle{Convergence Diagnostics for MICE}

%% - \Abstract{} almost as usual
\Abstract{
  This Research Report contains a simulation study that serves as the basis of the technical paper that will be submitted for publication in \emph{Journal of Statistical Software}. I have chosen to use the first format from the Research Report guidelines: ``\emph{It is written as a (mini) thesis, with an introduction, methods section, some results (i.e., preliminary analyses, or pilot simulations), and a discussion of results. The length of the research report should be maximally 2500 words of text (without references list and or tables and figures). Please do not include appendices, and no more than 6 tables or figures. Table and Figure captions do not count towards the word limit. An abstract may be included, but is not necessary}''. I aim to publish a pre-print of this research report on ArXiv (\url{https://arxiv.org/}). That way, I can refer to the simulation study described here, without 'bulking up' the technical paper that I want to submit for publication in JSS. Another option would be to attach this note as online appendix to the technical paper on ShinyMICE.
%"It is the first half of the thesis, i.e., there are no results included yet, but the report contains a full introduction including a literature review and a methods section that contains details about the data, instruments and or statistical procedures". The goal was to develop novel methodology and guidelines for evaluating multiple imputation methods, and implement these in an interactive evaluation framework for multiple imputation: \pkg{ShinyMICE}.
  }

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{multiple imputation, convergence, \pkg{mice}, \proglang{R}}
\Plainkeywords{multiple imputation, convergence, mice, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Hanne Ida Oberman, BSc.\\
  Methodology and Statistics for the Behavioural, Biomedical and Social Sciences\\
  Department of Methodology and Statistics\\
  Faculty of Social and Behavioral Sciences\\
  Utrecht University\\
  Heidelberglaan~15\\
  3500 Utrecht, The Netherlands\\
  E-mail: \email{h.i.oberman@uu.nl} % \\
  % URL: \url{https://eeecon.uibk.ac.at/~zeileis/}
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

\section{Introduction} \label{sec:intro} % curly brackets can handle typesetting

Feedback research seminar: order is not intuitive, mentioned, explained later. short sentences with a lot of content. implicitly a lot happens. expand sentence or it's obvious and i should delete it. Explain 'true data generating model'. follow important concepts with explanation or examples in next part. Look at abbreviations. for example r hat is just metnioned as 'e.g.' not clear in intro how important it is. also, look at distinction between mi and mice, they look like synonyms now. look at imputing vs analysing because they're called steps somewhere, and something else somewhere else. explain what a chain is. make clear when i talk about my study or lit. look at equations vehtari, because there is no real need for it. try to rephrase in my own words because of text flow. make clear that i only generate 1 dataset (not simulated data). in figure, r hat looks stable after 20 it. in discussion, explain that less it is advantage of mi, not a disadv. compared to MCMC. then final par is too negative, explain scope in intro instead.

At some point, any scientist conducting statistical analyses will run into a missing data problem \citep{alli02}. Missingness is problematic because statistical inference cannot be performed on incomplete data, and ad hoc solutions can yield wildly invalid results \citep{buur18}. To circumvent the ubiquitous problem of missing information, \cite{rubin87} proposed the framework of multiple imputation (MI). MI is an iterative algorithmic procedure in which missing data points are 'guessed' (i.e. imputed) several times. [Add some stuff here about MICE.] The variability between the imputations validly reflects how much uncertainty in the inference is due to missing information--that is, if all statistical assumptions are met \citep{rubin87}.
%Missing data is ubiquitous. MICE can solve stuff. 

With MI, many assumptions are made about the nature of the observed and missing parts of the data and their relation to the `true' \emph{data generating model} \citep{buur18}. Without proper evaluation of the imputations and the underlying assumptions, any drawn inference may erroneously be deemed valid. Such evaluation measures are currently missing or under-developed in MI software, like the world leading \proglang{R} package \pkg{mice} \citep{mice}. %Therefore, I will answer the following question: 'Which measures are vital for evaluating the validity of multiply imputed data?'.

A fundamental assumption of MICE is convergence of the algorithm. MICE is a sort of Gibbs sampler, a type of MCMC algorithm. In general, the validity of inference resulting from MCMC algorithms is threatened by non-convergence. So we use convergence diagnostics to flag non-convergence. But it is not known whether conventional convergence diagnostics for MCMC methods work on MI data. 

Conventional thresholds to diagnose non-convergence-- e.g., Gelman and Rubin's \citeyear{gelm92} statistic $\widehat{R} < 1.1$ --are not applicable on multiply imputed data \citep{lace07}. Therefore, empirical researchers have to rely on visual inspection procedures that are theoretically equivalent to $\widehat{R}$ \citep{whit11}. Visually assessing convergence is not only difficult to the untrained eye, it might also be futile. The convergence properties of MI algorithms lack scientific consensus \citep{taka17}, and some default MICE techniques might not converge to stable distributions at all \citep{murr18}. Moreover, convergence diagnostics for MI methods have not been systematically studied \citep{buur18}.

This paper investigates convergence properties of MI algorithms by means of model-based simulation in \proglang{R} \citep{R}. The results of this simulation study are guidelines for applied researchers to evaluate convergence of MI algorithms. %I will replicate Lacerda et al.'s simulation study on $\widehat{R}$ \citep{lace07}, and develop novel guidelines for assessing convergence. Ideally, I will integrate several diagnostics (e.g., $\widehat{R}$, \emph{auto-correlation}, and \emph{simulation error}) into a single summary indicator to flag non-convergence. 


%% -- Features ---------------------------------------------------------------
\subsection{Features} \label{sec:features}

The aim of this paper is to provide applied researchers a tool to assess convergence of MICE algorithms. The intended audience consists of empirical researchers and statisticians who use multiple imputation to solve missing data problems. Basic familiarity with multiple imputation methodology is assumed. For an accessible and comprehensive introduction to MI from an applied perspective, see \cite{buur18}. For the theoretical foundation of MI, see \cite{rubin87}. All programming code used in this paper is available in the file 'XYZ.R' along with the manuscript, and on Github repository 'XYZ'. 
% "The intended audience of this paper consists of applied researchers who want to address problems caused by missing data by multiple imputation. The text assumes basic familiarity with R. The document contains hands-on analysis using the mice package. We do not discuss problems of incomplete data in general. We refer to the excellent books by Little and Rubin (2002) and Schafer (1997). Theory and applications of multiple imputation have been developed in Rubin (1987) and Rubin (1996). van Buuren (2012) introduces multiple imputation from an applied perspective" \cite[p.~4]{mice}.


%% -- Notation ---------------------------------------------------------------
\subsection{Notation} \label{sec:notation}

The convergence guidelines introduced in this paper are developed to be integrated into the \pkg{mice} environment \citep{mice} in \proglang{R} \citep{R}. It therefore follows notation and conventions of \cite{mice}. For an overview of the deviations from the 'original' notation by \cite{rubin87}, see \cite{buur18}. 

Let $Y$ denote an $n \times p$ matrix containing the data values on $p$ variables for all $n$ units in a sample. The collection of observed data values is denoted as $Y_{obs}$; the missing part of $Y$ is referred to as $Y_{mis}$. Response indicator $R$ shows whether a data value in $Y$ is missing or observed. The relation between $R$, $Y_{obs}$, and $Y_{mis}$ determines the missingness mechanism. This paper only considers a 'missing completely at random' mechanism, where the probability to be missing is equal for all $n \times p$ cells in $Y$.

With multiple imputation, $Y_{mis}$ is 'filled in' (i.e. imputed) $m$ times. The $m$ imputed data sets are analyzed separately, and their results pooled. A distinction is made between the imputation step (employing $p$ 'imputation models'), and the analysis step (using one 'complete-data model' for each of the $m$ imputed data sets). The quantity of scientific interest is the result of the analysis step, e.g., a regression coefficient, and denoted with $Q$. That is, $Q$ is estimated on each imputed data set, resulting in $m$ separate $\hat{Q}$ values. These are combined into pooled estimate $\bar{Q}$.
% "Let Q denote the quantity of scientific interest (e.g., a regression coefficient)." \cite[p.~4]{mice}.

[Explain the use of `imputation chain' (the iterations of an imputation, only the final imputed values are used), `simulation condition' (the number of iterations the algorithm has before imputing), ...?]

% "Let Y denote the n x p matrix containing the data values on p variables for all n units in the sample. We define the response indicator R as an n x p 0-1 matrix. The elements of Y and R are denoted by y i j and r i j , respectively, where i = 1 , ... , n and j = 1 , ... , p . If y i j is observed, then r i j = 1 , and if y i j is missing, then r i j = 0 ." ... The observed data are collectively denoted by Y o b s . The missing data are collectively denoted as Y m i s , and contain all elements y i j where r i j = 0 . When taken together Y = ( Y o b s , Y m i s ) contain the hypothetically complete data." \cite[par.~2.2]{buur18}
% The data are said to be MCAR if Pr ( R = 0 | Y o b s , Y m i s , $\psi$ ) = Pr ( R = 0 | $\psi$ ) (2.1) so the probability of being missing depends only on some parameters $\psi$ , the overall probability of being missing. The data are said to be MAR if Pr ( R = 0 | Y o b s , Y m i s , $\psi$ ) = Pr ( R = 0 | Y o b s , $\psi$ ) (2.2) so the missingness probability may depend on observed information, including any design factors. Finally, the data are MNAR if Pr ( R = 0 | Y o b s , Y m i s , $\psi$ ) (2.3) does not simplify, so here the probability to be missing also depends on unobserved information, including Y m i s itself. \cite[par.~2.2]{buur18}

% Also introduce: 
% 
% - Terminology (MCAR, MAR, MNAR)?
% 
% - Blue points are observed, the red points are imputed?

%% -- Theoretical Background ---------------------------------------------------------------
\section{Theoretical Background} \label{sec:background}

Convergence has two interpretations [look up source!!!]: mixing between chains and stability over iterations within chains. Ideally, we want the chains to intermingle nicely, without trends within chains. 
% "In order to converge to a stationary distribution, a Markov chain needs to satisfy three important properties (Roberts 1996; Tierney 1996): irreducible, the chain must be able to reach all interesting parts of the state space; aperiodic, the chain should not oscillate between different states; recurrence, all interesting parts can be reached infinitely often, at least from almost all starting points. Do these properties hold for the MICE algorithm? Irreducibility is generally not a problem since the user has large control over the interesting parts of the state space. This flexibility is actually the main rationale for FCS instead of a joint model. Periodicity is a potential problem, and can arise in the situation where imputation models are clearly inconsistent. A rather artificial example of an oscillatory behavior occurs when Y 1 is imputed by Y 2 $\beta$ + $\epsilon$  1 and Y 2 is imputed by - Y 1 $\beta$ + $\epsilon$  2 for some fixed, nonzero $\beta$ . The sampler will oscillate between two qualitatively different states, so the correlation between Y 1 and Y 2 after imputing Y 1 will differ from that after imputing Y 2 . In general, we would like the statistical inferences to be independent of the stopping point. A way to diagnose the ping-pong problem, or order effect, is to stop the chain at different points. The stopping point should not affect the statistical inferences. The addition of noise to create imputations is a safeguard against periodicity, and allows the sampler to "break out" more easily. Non-recurrence may also be a potential difficulty, manifesting itself as explosive or non-stationary behavior. For example, if imputations are made by deterministic functions, the Markov chain may lock up. Such cases can sometimes be diagnosed from the trace lines of the sampler. See Section 6.5.2 for an example. As long as the parameters of imputation models are estimated from the data, non-recurrence is mild or absent. The required properties of the MCMC method can be translated into conditions on the eigenvalues of the matrix of transition probabilities (MacKay 2003, 372-73). The development of practical tools that put these conditions to work for multiple imputation is still an ongoing research problem." \cite[par.~4.5]{buur18}
We can never be certain of convergence, therefore convergence diagnostics evaluate signs of non-convergence \citep{hoff09}. % In general: "In general, you cannot know for sure if your chain has converged. But sometimes you can know if your chain has not converged, so we at least check for this latter possibility" \cite[p.~101]{hoff09}
Diagnose non-convergence in the mixing sense: $\widehat{R}$. Diagnose non-convergence in the stability sense: auto-correlation. 

In the latter interpretation of convergence, convergence is interpreted as stability over iterations, or non-recurring. Non-recurrence can be evaluated with auto-correlation. Auto-correlation shows how dependent subsequent draws of an imputation chain are on the previous value. If there is a lot of dependence, draws at e.g. iteration five are significantly correlated with the value of the first draw. Auto-correlation above the confidence level indicate dependence within chains. Also, possible to use MC error, Geweke, and the other one. But outside of scope of this study. Here, the focus is mainly on $\widehat{R}$.

The potential scale reduction factor $\widehat{R}$ tells us how much variance of an estimate could be shrunken down by running the chains infinitely long. That then tells us something about how dependent the chains are on the starting values. If there is no dependence on the initial values anymore, the chains have converged (in the mixing sense of the word). $\widehat{R}$ is then equal to one. Conventional threshold was $\widehat{R} < 1.2$ is acceptable. Most recent guideline by \cite{veht19} is more stringent: $\widehat{R} < 1.01$. 

Why is $\widehat{R}$ not applicable on MI data? $\widehat{R}$ is not appropriate because it assumes over-dispersed initial values, which means that the initial values of the $m$ imputation chains are 'far away' from the distribution that the chains are converging to. With MI procedures, initial values are chosen such that ... [or maybe even estimated from the observed data??]. This means that at most one initial value is necessary, but probably none at all. [I should look this up.] Without over-dispersed initial states, $\widehat{R}$ can falsely diagnose convergence \citep{broo98}. %would not be sensitive enough to flag non-convergence of MI algorithms: "if over-dispersion does not hold, $\sigma_+^2$ can be too low, which can lead to falsely diagnosing convergence." \cite[p~437]{broo98}. 
This suggests that $\widehat{R}$ would not be sensitive enough to flag non-convergence of MI algorithms. Empirical finding, however, show that the opposite may be true: \cite{lace07} report $\widehat{R}$ values above the threshold of $\widehat{R} < 1.1$ after fifty iterations. %that $\widehat{R}$ will not be smaller than 1.1 before iteration number 50. Therefore, the aim is to replicate empirical finding Lacerda et al. 
Hypothesis of this simulation study is that $\widehat{R}$ will over-estimate non-convergence. [Or use:] Hypothesis based on \cite{lace07} is that the conventional acceptable level of $\widehat{R}$ is too strict for MI data. We expect that the simulation diagnostics will indicate valid inference before $\widehat{R}$ will. 

% Also introduce:
% 
% - missingness mechanims, ignorability?
% % "The practical importance of the distinction between MCAR, MAR and MNAR is that it clarifies the conditions under which we can accurately estimate the scientifically interesting parameters without the need to know $\psi$" \cite[par.~2.2]{buur18}.
% 
% - Rubin's rules?
% 
% - FCS vs. JM?
% 


%% -- Methods ---------------------------------------------------------------

%%
%%
%%
%%

\section{Methods} \label{sec:methods}

This research project consists of an investigation into algorithmic convergence of MI algorithms. I will replicate Lacerda et al.'s simulation study on $\widehat{R}$ \citep{lace07}, and develop novel guidelines for assessing convergence. Ideally, I will integrate several diagnostics (e.g., $\widehat{R}$, \emph{auto-correlation}, and \emph{simulation error}) into a single summary indicator to flag non-convergence. 

The aim is to evaluate whether the imputation procedure has converged. The primary research interest is in determining whether $\widehat{R}$ is an appropriate convergence diagnostic, and if so, which level of stringency suits MI data. We can apply $\widehat{R}$ to the mean (or to the first two moments) of the variables of interest. The simulation diagnostics are as recommended by \cite{buur18}. Namely, average bias, average confidence interval width, and empirical coverage rate (coverage probability) across simulations. %"We expect that in accordance with Neyman (1934), at least 95 percent of the confidence intervals should contain the true population value, although some room for simulation error should be taken into account to counteract the finite nature of our simulations"\cite{schou18}.
Also look at distributional characteristics, and plausibility of imputed values, see \cite{vinknd}. % Or use: Then evaluate the regular diagnostics of MI simulations, see Vink, n.d., and  convergence. Diagnostics include absolute bias of the estimated regression coefficient, confidence interval width, and empirical coverage rate across simulations. Convergence is evaluated with $\widehat{R}$, or potential scale reduction factor.

% Distributional characteristics: In practice, the distribution of the incomplete data may differ greatly from the observed data. Under anything but the MCAR assumption, this can be expected. When evaluating imputations, the distributional shapes should be checked and diagnostic evaluations should be performed (see Abayomi et al., 2008, for an detailed overview of diagnostic evaluation for multivariate imputations). When anomalies are found, and if the imputation method is valid, there should be an explanation, especially in the controlled environment of a properly executed simulation study. 

% Plausibility of the imputed values: Plausible imputations - imputations that could be real values if they had been observed - are not a necessary condition for obtaining valid inference. However, in practice, especially when the imputer and the analyst are different persons, plausibility of imputations may be a desired property. When evaluating imputation routines, the evaluator should mention whether the routine is prone to deliver implausible value

Convergence diagnostics are $\widehat{R}$ and auto-correlation (at lag one). $\widehat{R}$ is computed as follows: % The scalar summary of interest in this study is the average of the imputed values per variable. %As recommended by \cite{veht19} $\widehat{R}$ is computed as .... to be able to detect ... in the tails of the distribution. \cite[p.~5]{veth19}

\begin{quotation}
``In the equations below, $N$ is the number of draws per chain, $M$ is the number of chains, and $S = MN$ is the total number of draws from all chains. For each scalar summary of interest $\theta$, we compute $B$ and $W$, the between- and within-chain variances:
%
\begin{align*}
B=\frac{N}{M-1} \sum_{m=1}^{M}\left(\bar{\theta}^{(\cdot m)}-\bar{\theta}^{(\cdot \cdot)}\right)^{2}, \text { where } \bar{\theta}^{(\cdot m)}=\frac{1}{N} \sum_{n=1}^{N} \theta^{(n m)}, \quad \bar{\theta}^{(\cdot \cdot)}=\frac{1}{M} \sum_{m=1}^{M} \bar{\theta}^{(\cdot m)} \\
W=\frac{1}{M} \sum_{m=1}^{M} s_{j}^{2},  \text { where } s_{m}^{2}=\frac{1}{N-1} \sum_{n=1}^{N}\left(\theta^{(n m)}-\bar{\theta}^{(\cdot m)}\right)^{2}". \text{ \cite[p.~5]{veht19}} 
\end{align*}
%
\end{quotation}

The proportion of within chain variance $W$ against the weighted average of $B$ and $W$, $\widehat{\operatorname{var}}^{+}$ tells us how much variance of $\theta$ could be shrunken down if $N\to\infty$. $\widehat{\operatorname{var}}^{+}$ is computed as follows: %over-estimates the marginal posterior variance of the estimand, $\theta$. 

\begin{equation*}
\widehat{\operatorname{var}}^{+}(\theta | y)=\frac{N-1}{N} W+\frac{1}{N} B.
\end{equation*}

The potential scale reduction factor $\widehat{R}$ is obtained as:

\begin{equation*}
\widehat{R}=\sqrt{\frac{\widehat{\operatorname{var}}^{+}(\theta | y)}{W}}.
\end{equation*}

In this study, $\widehat{R}$ is computed for the mean of each variable, in each imputation chain, over all simulation conditions and MCMC simulations. %For each simulation in each simulation condition, the maximum value of $\widehat{R}$ is reported.


% \subsubsection{What is already implemented?}
% 
% - Trace-plots
% 
% \subsubsection{What is not yet implemented, but exists?}
% 
% - $\widehat{R}$, but too stringent (new) threshold, and assumption of over-dispersed initial values of imputation chains not met.
% % Potentially add something about updated (2019) version of $\widehat{R}$ and the new threshold of 1.01, see \citep{veht19}.
% 
% - Auto-correlation. Schafer (1997, p. 129) wrote on worst linear statistic. We could calculate the auto-correlation of that statistic to know that the algorithm converged elsewhere too. See autocorr function plot in SAS of worst linear function.
% % Worst linear function in SAS see \url{https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_mi_sect027.htm}.
% % Note: we're talking about missing data only, not the combined data (that autocorrelation is very high, as is the autocorrelation of deductively imputed values, like in the texp example). 
% % "Applications of MICE with lowly correlated data therefore inject a lot of noise into the system. Hence, the auto-correlation over t will be low, and convergence will be rapid, and in fact immediate if all variables are independent. Thus, the incorporation of noise into the imputed data has pleasant side-effect of speeding up convergence" \citep{buur18}, par. 4.5. 
% 
% - Sensitivity analysis: Run algorithm several times and compare results. 
% % "Imputers who do choose to use FCS should use flexible univariate models wherever possible and take care to assess apparent convergence of the algorithm, for example by computing traces of pooled estimates or other statistics and using standard MCMC diagnostics (Gelman et al., 2013, Chapter 11). It may also be helpful to examine the results of many independent runs of the algorithm with different initializations and to use random scans over the p variables to try to identify any convergence issues and mitigate possible order dependence" \cite[p.~19]{murr18}.
% 
% \subsubsection{What is not implemented, and NA?}
% 
% - $\widehat{R}$ threshold: Replicate simulation study and build a decision rule to solve the problem with $\widehat{R}$.
% % "The monitoring statistic was computed for mean monthly earnings at each iteration in chains of length k = 200. Since calculation of the statistic requires M parallel sequences, m = 5 such chains were constructed. This value of m was informed by the preferred choice given in the literature on multiple imputation. The monitoring statistic computed at each iteration is presented in Figures 15 to 17 for each of the three missingness mechanisms. The red vertical line denotes ten iterations" \cite[p.~49]{lace07}.
% 
% - Stability of the solution: Possibly use the slope of means over iterations too to see whether there is trending. Or apply PCA on the imputed data and if that (the eigenvalues?) stays the same we know that the means and variances are stable as well, see McKay (?). 
% 
% - MC error: MC error = SD/sqrt(number of iterations), where SD represents the variation across iterations. The MC error thus represents how much the means differ w.r.t. the iterations. MC error decreases as number of iterations increases. It should not be larger than 5\% of the sample standard deviation.

%% -- Simulation Approach ---------------------------------------------------------------

%%
%%
%%
%%

\section{Simulation Approach}

In this study, 1000 MCMC simulations are performed. The simulation set-up consists of several steps, see pseudo-code below. Simulation code is available in online appendix XYZ on Github.

\begin{Code}
# pseudo-code of simulation 
simulate data with missingness
for (number of simulation runs from 1 to 1000)
  for (number of iterations from 1 to 100)
    impute the missingness
    compute convergence diagnostics
    perform analysis
    pool results
    compute simulation diagnostics
aggregate convergence and simulation diagnostics
\end{Code}

The simulated data is a finite population of $N=1000$. The variables are dependent variable $Y$, independent variable $X$, and covariates $Z_1$ and $Z_2$. The quantity of scientific interest is the estimated regression coefficient of $X$ on $Y$ in linear regression model $Y \sim X+Z_1+Z_2$. The data generating model of the predictors is a multivariate normal distribution with means structure $\mu$, and variance-covariance matrix $\Sigma$. Outcome variable $Y$ is deduced from the predictor variables as follows:

\begin{align*}
\begin{pmatrix}X\\
Z_{1}\\
Z_{2}\\
\epsilon
\end{pmatrix} &\sim  N
\begin{bmatrix}
\begin{pmatrix}
12\\
3\\
0.5\\
0
\end{pmatrix}\!\!,&
\begin{pmatrix}
4 & 4 & 1.8 & 0\\
4 & 16 & 4.8 & 0\\
1.8 & 4.8 & 9 & 0\\
0 & 0 & 0 & 100
\end{pmatrix}
\end{bmatrix}\\[2\jot]
Y &=  2 \times X + 0.5 \times Z_1 - 1 \times Z_2 + \epsilon
\end{align*}

After data generation, the complete data is 'amputed'. That is, the \pkg{mice} function \fct{ampute} in \proglang{R} is used to impose an MCAR missingness mechanism upon the data. The probability to be missing is the same for all cells, namely XYZ\%. %"Furthermore, we generate three kinds of missingness proportions: 0.1, 0.5, and 0.9. Note that these values indicate the sampled proportion of incomplete cases in Y" \cite{schou18}.
Table XYZ shows a summary of the generated complete data, and the amputed data. The amputed data is the starting point of each of the 1000 simulations per simulation condition.

Missing data points are imputed with \pkg{mice} in \proglang{R}. All simulations are performed with imputation method 'norm' (Bayesian linear regression imputation), and five imputation chains ($m = 5$). The number of iterations is varied over simulations ('maxit' argument between 1 and 100). Each simulation condition (i.e., each number of iterations) is simulated 1000 times. Simulation and convergence diagnostics are aggregated over the 1000 MCMC simulations. 
% Missingness mechanism: "With MCAR missingness mechanisms, the probability to be missing is the same for all cases. This is a necessary simulation condition for evaluating the performance of imputation procedures. If an imputation method is not able to solve the problem (i.e. yield valid inference) under MCAR, the statistical properties of the procedure are not sound." (Vink, n.d., p. 4)
% "combine the m completed data inferences into a single inference following Rubin's (1987) rules (pp. 76-77)" \cite{schou18}.


Add ``post-processing steps (e.g. aggregation computations, summary statistics, regressions on the simulation results) used to transform simulation outputs to reported results''. %, see \url{http://jsterman.scripts.mit.edu/docs/Rahmandad-Sterman%20Simulation%20Reporting%20Standards3.30%20update.pdf}.

Post-processing of simulation diagnostics:

Bias is computed as the estimated regression coefficient after MI minus the true regression coefficient. Bias is averaged over all simulations with the same simulation condition.

Confidence interval width (CIW) is computed as the diffence between the lower and upper bound of the 95\% confidence interval (CI95\%). The CI95\% bounds are computed as the estimated regression coefficient plus or minus (respectively) the pooled SE across imputations times 2.66 (the quantile of a t distribution with m-1 degrees of freedom). CIW is averaged over all simulations with the same simulation condition.

Coverage rate is computed as the proportion of simulations (with the same simulation condition) in which the true regression coefficient is between the bounds of the CI95\%. 

Post-processing of convergence diagnostics:

$\widehat{R}$ is computed within imputation chains: for each variable, for each simulation condition, for each simulation. The maximum value across variables within the same simulation is reported. Maximum $\widehat{R}$ values per simulation condition are aggregated across simulations.

Autocorrelation (AC) is computed within imputation chains, as the correlation between $\theta$ at the $i^{th}$ iteration and $\theta$ at the $i+1^{th}$ iteration. For now, we only consider the chain mean as scalar of interest $\theta$. The AC of the variable with the highest absolute AC value is reported per simulation. These values are then averaged per simulation condition.


%% -- Results ---------------------------------------------------------------

%%
%%
%%
%%

\section{Results}

Figure \ref{resultssim} shows the results over 1000 simulations. A subset of simulation conditions is presented in Table \ref{subsetresults}. 

\begin{figure}[h]
  \resizebox{\textwidth}{!}{ %notice the \resizebox{} command
        \includegraphics{res3.pdf}
  }      
  \caption{Simulation and convergence diagnostics over 1000 MCMC simulations.}
    \label{resultssim}
\end{figure}

Evaluate simulation diagnostics one by one: bias, empirical SE, confidence interval width, and coverage rate. Then evaluate convergence diagnostics $\widehat{R}$ and auto-correlation.

%% --------------------------------
\subsection{Simulation diagnostics}
Bias. After 1 or 2 iterations bias fluctuates within a narrow range of the average bias across all simulation conditions.

Empirical standard error (SE).


Confidence interval width. Does not show a trend across the simulation conditions. [Is related to empirical SE??]


Coverage rate. Is more or less stable after one iteration. But higher percentages then the expected 95\% \cite{neym43}.

%% --------------------------------
\subsection{Convergence diagnostics}

$\widehat{R}$. Steep drop down up-to iteration 30-40, then stable. Conventional threshold $\widehat{R} < 1.2$ reached after three iterations, $\widehat{R} < 1.1$ after four iterations, $\widehat{R} < 1.01$ after 38 iterations (see online appendix). 

Auto-correlation. Auto-correlation is dangerous when positive. These auto-correlations are negative. Still, we want the iterations to be stable. Low auto-correlations 


% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Tue Nov 12 17:43:11 2019
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrrr}
  \hline
  &\multicolumn{4}{c}{Simulation Diagnostics}& & \multicolumn{2}{c}{Convergence Diagn.} \\
  \cline{2-5} \cline{7-8}
It. & Bias & Emp. SE & CI width & Cov. rate & & $\widehat{R}$ & Auto-corr. \\ 
  \hline
  1 & -0.135 & 0.115 & 0.637 & 0.930 &  & & \\ 
  2 & -0.088 & 0.125 & 0.691 & 0.990 & & 1.499 & -0.500 \\ 
  3 & -0.090 & 0.112 & 0.623 & 0.950 & &  1.200 & -0.658 \\ 
  4 & -0.093 & 0.116 & 0.645 & 0.980 &  & 1.146 & -0.738 \\ 
  5 & -0.087 & 0.108 & 0.599 & 0.980 &  & 1.098 & -0.711 \\ 
  6 & -0.092 & 0.116 & 0.644 & 0.950 & &  1.090 & -0.674 \\ 
  7 & -0.095 & 0.122 & 0.675 & 1.000 & &  1.063 & -0.588 \\ 
  8 & -0.099 & 0.113 & 0.626 & 0.970 & &  1.058 & -0.523 \\ 
  9 & -0.095 & 0.115 & 0.639 & 1.000 & &  1.044 & -0.519 \\ 
  10 & -0.090 & 0.111 & 0.618 & 0.990 & &  1.048 & -0.414 \\ 
  15 & -0.093 & 0.109 & 0.604 & 0.950 & &  1.020 & -0.339 \\ 
  20 & -0.098 & 0.109 & 0.606 & 0.980 & &  1.023 & -0.234 \\ 
  25 & -0.100 & 0.113 & 0.626 & 0.990 &  & 1.016 & -0.091 \\ 
  30 & -0.091 & 0.118 & 0.654 & 0.960 & &  1.014 & -0.153 \\ 
  40 & -0.088 & 0.117 & 0.650 & 0.990 &  & 1.009 & -0.070 \\ 
  50 & -0.087 & 0.119 & 0.662 & 0.990 &  & 1.009 & -0.029 \\ 
  % 11 & -0.100 & 0.127 & 0.704 & 0.970 & 1.046 & -0.360 \\ 
  % 12 & -0.096 & 0.115 & 0.641 & 0.980 & 1.032 & -0.354 \\ 
  % 13 & -0.098 & 0.120 & 0.664 & 0.960 & 1.034 & -0.328 \\ 
  % 14 & -0.094 & 0.113 & 0.630 & 0.990 & 1.030 & -0.353 \\ 
  % 15 & -0.093 & 0.109 & 0.604 & 0.950 & 1.020 & -0.339 \\ 
  % 16 & -0.094 & 0.114 & 0.630 & 0.980 & 1.025 & -0.272 \\ 
  % 17 & -0.091 & 0.116 & 0.645 & 0.980 & 1.028 & -0.243 \\ 
  % 18 & -0.091 & 0.116 & 0.645 & 0.990 & 1.023 & -0.179 \\ 
  % 19 & -0.087 & 0.114 & 0.630 & 0.980 & 1.024 & -0.281 \\ 
  % 20 & -0.098 & 0.109 & 0.606 & 0.980 & 1.023 & -0.234 \\ 
  % 21 & -0.102 & 0.116 & 0.646 & 0.960 & 1.017 & -0.192 \\ 
  % 22 & -0.095 & 0.114 & 0.633 & 0.980 & 1.016 & -0.119 \\ 
  % 23 & -0.088 & 0.107 & 0.595 & 0.980 & 1.017 & -0.246 \\ 
  % 24 & -0.097 & 0.112 & 0.621 & 0.980 & 1.016 & -0.148 \\ 
  % 25 & -0.100 & 0.113 & 0.626 & 0.990 & 1.016 & -0.091 \\ 
  % 26 & -0.096 & 0.113 & 0.628 & 0.980 & 1.017 & -0.177 \\ 
  % 27 & -0.096 & 0.117 & 0.650 & 0.980 & 1.014 & -0.167 \\ 
  % 28 & -0.102 & 0.116 & 0.645 & 0.960 & 1.015 & -0.038 \\ 
  % 29 & -0.098 & 0.114 & 0.632 & 0.990 & 1.016 & -0.137 \\ 
  % 30 & -0.091 & 0.118 & 0.654 & 0.960 & 1.014 & -0.153 \\ 
  % 31 & -0.098 & 0.115 & 0.639 & 0.980 & 1.012 & -0.101 \\ 
  % 32 & -0.096 & 0.113 & 0.626 & 0.940 & 1.011 & -0.095 \\ 
  % 33 & -0.089 & 0.117 & 0.652 & 0.970 & 1.011 & -0.088 \\ 
  % 34 & -0.093 & 0.115 & 0.639 & 0.960 & 1.013 & -0.061 \\ 
  % 35 & -0.103 & 0.115 & 0.638 & 0.970 & 1.011 & -0.126 \\ 
  % 36 & -0.090 & 0.113 & 0.630 & 0.980 & 1.010 & -0.161 \\ 
  % 37 & -0.098 & 0.122 & 0.675 & 0.990 & 1.011 & -0.070 \\ 
  % 38 & -0.095 & 0.118 & 0.656 & 0.970 & 1.011 & -0.119 \\ 
  % 39 & -0.091 & 0.121 & 0.670 & 0.980 & 1.009 & -0.100 \\ 
  % 40 & -0.088 & 0.117 & 0.650 & 0.990 & 1.009 & -0.070 \\ 
  % 41 & -0.095 & 0.117 & 0.648 & 0.960 & 1.010 & -0.148 \\ 
  % 42 & -0.095 & 0.118 & 0.657 & 0.990 & 1.009 & -0.117 \\ 
  % 43 & -0.095 & 0.116 & 0.645 & 0.980 & 1.009 & -0.055 \\ 
  % 44 & -0.101 & 0.125 & 0.692 & 0.980 & 1.009 & -0.068 \\ 
  % 45 & -0.085 & 0.117 & 0.651 & 0.980 & 1.008 & -0.086 \\ 
  % 46 & -0.092 & 0.117 & 0.649 & 0.970 & 1.008 & -0.048 \\ 
  % 47 & -0.095 & 0.115 & 0.638 & 0.980 & 1.007 & -0.069 \\ 
  % 48 & -0.096 & 0.112 & 0.620 & 0.970 & 1.009 & -0.076 \\ 
  % 49 & -0.090 & 0.108 & 0.601 & 0.980 & 1.009 & -0.083 \\ 
  % 50 & -0.087 & 0.119 & 0.662 & 0.990 & 1.009 & -0.029 \\ 
   \hline
\end{tabular}
\caption{Simulation and convergence diagnostics over 1000 MCMC simulations.}
\label{subsetresults}
\end{table}

From the simulation diagnostics it appears that as little as three iterations is sufficient to draw valid inference. Convergence diagnostics $\widehat{R}$ and auto-correlation, however, indicate 20-30 iterations. Apparently, they are not perfect for MI data [move interpretation to Discussion!]. 

[Add simulation time (computational cost) to table? Add empirical SE of all diagnostics? Sample some random simulations and check distribution and plausibility of imputations? Use chain variance as scalar of interest for rhat and autocorrelation?]

%% -- Manuscript ---------------------------------------------------------------



%% -- Illustrations ------------------------------------------------------------



%% -- Summary/conclusions/discussion -------------------------------------------
%%
%%
%%
%%

\section{Summary and discussion} \label{sec:summary}

This note illustrates that conventional convergence diagnostics behave differently on MI data as compared to the output of 'regular' MCMC algorithms. The recommended threshold for $\widehat{R}$ is too stringent for MI data.

Moreover, $\widehat{R}$ could theoretically not be smaller than one, yet it happened several times in this study (see online appendix XYZ). How could $\widehat{R}$ smaller than 1 occur? The number of simulations is smaller than in `regular' MCMC processes. Therefore, the '$(n-1/n)$' correction factor can influence the estimated potential scale reduction factor. This downwards bias is in the opposite direction than expected: ``The mixture-of-sequences variance, V, should stabilize as a function of n. (Before convergence, we expect $\sigma^2$ to decrease with n, only increasing if the sequences explore a new area of parameter space, which would imply that the original sequences were not overdispersed for the particular scalar summary being monitored.)'' \cite[p~438]{broo98}.

We would like to have convergence measures for multivariable statistics (scalars?) of interest. This is, however, dependent of the complete data model. The eigenvector decomposition method proposed by McKay (?) should be implemented. I could not find any resources to apply this method and it is outside the scope of this thesis to investigate how this approach could be implemented.


%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

The results in this paper were obtained using \proglang{R}~3.6.1 with the \pkg{mice}~3.6.0.9000 package. \proglang{R} itself and all packages used are available from the Comprehensive \proglang{R} Archive Network (CRAN) at \url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}

This paper is written by the sole author (Hanne Oberman, BSc.), with guidance from Master thesis supervisors prof. dr. Stef van Buuren, and dr. Gerko Vink.

%% -- Bibliography -------------------------------------------------------------
%% - References need to be provided in a .bib BibTeX database.
%% - All references should be made with \cite, \citet, \citep, \citealp etc.
%%   (and never hard-coded). See the FAQ for details.
%% - JSS-specific markup (\proglang, \pkg, \code) should be used in the .bib.
%% - Titles in the .bib should be in title case.
%% - DOIs should be included where available.

\bibliography{ShinyMICE}


%% -- Appendix (if any) --------------------------------------------------------
%% - After the bibliography with page break.
%% - With proper section titles and _not_ just "Appendix".

\newpage

\begin{appendix}

\end{appendix}

%% -----------------------------------------------------------------------------


\end{document}
